{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Sff0gpVTfNb"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1. Setup: install deps, load config, init secrets & OpenAI\n",
        "#    (GitHub Actions-ready: NO Colab upload, NO hardcoded keys)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pathlib\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# In GitHub Actions, these env vars come from repo secrets:\n",
        "#   OPENAI_API_KEY\n",
        "#   SUNO_API_KEY\n",
        "#   SUNO_BASE_URL   (we will set it as a secret too)\n",
        "#   YOUTUBE_CLIENT_SECRETS_JSON\n",
        "#   YOUTUBE_TOKEN_JSON\n",
        "# Optional later (but not required): YOUTUBE_PRIVACY\n",
        "\n",
        "def _get_env_required(name: str) -> str:\n",
        "    value = os.environ.get(name)\n",
        "    if not value:\n",
        "        raise ValueError(\n",
        "            f\"Missing required environment variable: {name}.\\n\"\n",
        "            \"You must add it as a GitHub repository secret and map it in the workflow.\"\n",
        "        )\n",
        "    return value\n",
        "\n",
        "def _get_env_optional(name: str, default: Optional[str] = None) -> Optional[str]:\n",
        "    return os.environ.get(name, default)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Core APIs\n",
        "    openai_api_key: str\n",
        "    suno_api_key: str\n",
        "    suno_base_url: str\n",
        "\n",
        "    # YouTube auth (stored as JSON strings in GitHub secrets)\n",
        "    youtube_client_secrets_json: str\n",
        "    youtube_token_json: str\n",
        "\n",
        "    # Misc\n",
        "    youtube_scope: str = \"https://www.googleapis.com/auth/youtube.upload\"\n",
        "    youtube_privacy: str = \"public\"\n",
        "\n",
        "def load_config() -> Config:\n",
        "    return Config(\n",
        "        openai_api_key=_get_env_required(\"OPENAI_API_KEY\"),\n",
        "        suno_api_key=_get_env_required(\"SUNO_API_KEY\"),\n",
        "        suno_base_url=_get_env_required(\"SUNO_BASE_URL\"),\n",
        "        youtube_client_secrets_json=_get_env_required(\"YOUTUBE_CLIENT_SECRETS_JSON\"),\n",
        "        youtube_token_json=_get_env_required(\"YOUTUBE_TOKEN_JSON\"),\n",
        "    )\n",
        "\n",
        "def write_youtube_secret_files(cfg: Config) -> None:\n",
        "    \"\"\"\n",
        "    Turn the JSON strings from env into the files expected by the YouTube client code:\n",
        "      - client_secrets.json\n",
        "      - token.json\n",
        "    \"\"\"\n",
        "    client_path = pathlib.Path(\"client_secrets.json\")\n",
        "    token_path = pathlib.Path(\"token.json\")\n",
        "\n",
        "    # Validate JSON to fail early if secrets were pasted incorrectly\n",
        "    try:\n",
        "        json.loads(cfg.youtube_client_secrets_json)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(\"YOUTUBE_CLIENT_SECRETS_JSON is not valid JSON. Re-paste it into GitHub Secrets.\") from e\n",
        "\n",
        "    try:\n",
        "        token_data = json.loads(cfg.youtube_token_json)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(\"YOUTUBE_TOKEN_JSON is not valid JSON. Re-paste it into GitHub Secrets.\") from e\n",
        "\n",
        "    # Must have refresh_token for unattended runs\n",
        "    if \"refresh_token\" not in token_data:\n",
        "        raise ValueError(\n",
        "            \"YOUTUBE_TOKEN_JSON is missing 'refresh_token'.\\n\"\n",
        "            \"You must generate token.json with offline access + youtube.upload scope.\"\n",
        "        )\n",
        "\n",
        "    client_path.write_text(cfg.youtube_client_secrets_json, encoding=\"utf-8\")\n",
        "    token_path.write_text(cfg.youtube_token_json, encoding=\"utf-8\")\n",
        "\n",
        "    print(f\"âœ… Wrote YouTube secrets to {client_path} and {token_path}\")\n",
        "\n",
        "# Load config\n",
        "config = load_config()\n",
        "\n",
        "# Initialize OpenAI client\n",
        "os.environ[\"OPENAI_API_KEY\"] = config.openai_api_key\n",
        "openai_client = OpenAI(api_key=config.openai_api_key)\n",
        "\n",
        "# Write YouTube secret files for later blocks\n",
        "write_youtube_secret_files(config)\n",
        "\n",
        "print(\"âœ… Config loaded successfully.\")\n",
        "print(\"   - OPENAI_API_KEY: present\")\n",
        "print(\"   - SUNO_API_KEY: present\")\n",
        "print(f\"   - SUNO_BASE_URL: {config.suno_base_url}\")\n",
        "print(\"   - YOUTUBE_CLIENT_SECRETS_JSON: present\")\n",
        "print(\"   - YOUTUBE_TOKEN_JSON: present\")\n",
        "print(f\"   - Default YouTube privacy: {config.youtube_privacy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2. Preflight checks: OpenAI, Suno, YouTube, ffmpeg\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import subprocess\n",
        "import requests\n",
        "from google.oauth2.credentials import Credentials\n",
        "from google.auth.transport.requests import Request\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import json\n",
        "\n",
        "# We reuse: config, openai_client, client_secrets.json, token.json\n",
        "\n",
        "\n",
        "# â”€â”€ OpenAI preflight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def preflight_openai(client: OpenAI):\n",
        "    \"\"\"\n",
        "    Simple, cheap call to verify the OpenAI API key works.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Testing OpenAI connectivity...\")\n",
        "    try:\n",
        "        models = list(client.models.list())\n",
        "        print(f\"âœ… OpenAI OK â€“ {len(models)} models visible.\")\n",
        "    except Exception as e:\n",
        "        print(\"âŒ OpenAI preflight failed.\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# â”€â”€ Suno preflight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def preflight_suno(cfg):\n",
        "    \"\"\"\n",
        "    Light sanity-check for Suno-style API.\n",
        "    We only check that:\n",
        "      - A base URL exists\n",
        "      - We can reach it without network/SSL errors.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Testing Suno connectivity...\")\n",
        "\n",
        "    base_url = cfg.suno_base_url\n",
        "    if not base_url:\n",
        "        raise ValueError(\n",
        "            \"Suno base URL is not set.\\n\"\n",
        "            \"Set SUNO_BASE_URL in Colab (Block 1) or as a GitHub secret later.\\n\"\n",
        "            \"Example: https://api.sunoapi.org/api/v1\"\n",
        "        )\n",
        "\n",
        "    base_url = base_url.rstrip(\"/\")\n",
        "    headers = {\"Authorization\": f\"Bearer {cfg.suno_api_key}\"}\n",
        "\n",
        "    try:\n",
        "        # Very cheap connectivity check:\n",
        "        resp = requests.get(base_url, headers=headers, timeout=10)\n",
        "        print(f\"âœ… Suno connectivity OK â€“ HTTP {resp.status_code} from {base_url}\")\n",
        "    except Exception as e:\n",
        "        print(\"âŒ Suno preflight failed.\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# â”€â”€ YouTube auth & preflight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "SCOPE = config.youtube_scope  # same as we defined in Config\n",
        "\n",
        "\n",
        "def get_authenticated_service():\n",
        "    \"\"\"Create authenticated YouTube client with scope validation.\"\"\"\n",
        "    try:\n",
        "        # Load token data from the file we wrote in Block 1\n",
        "        with open(\"token.json\") as f:\n",
        "            token_data = json.load(f)\n",
        "\n",
        "        # Verify the token has our required scope\n",
        "        if \"scopes\" not in token_data or SCOPE not in token_data[\"scopes\"]:\n",
        "            raise ValueError(f\"Token missing required scope: {SCOPE}\")\n",
        "\n",
        "        creds = Credentials.from_authorized_user_info(token_data, [SCOPE])\n",
        "\n",
        "        # Refresh token if needed\n",
        "        if creds and creds.expired and creds.refresh_token:\n",
        "            try:\n",
        "                creds.refresh(Request())\n",
        "                # Update token file with refreshed credentials\n",
        "                with open(\"token.json\", \"w\") as f:\n",
        "                    json.dump(json.loads(creds.to_json()), f)\n",
        "                print(\"ğŸ”„ YouTube token refreshed.\")\n",
        "            except Exception as refresh_error:\n",
        "                print(f\"âš ï¸ Token refresh failed: {refresh_error}\")\n",
        "                if not creds.token:\n",
        "                    raise\n",
        "\n",
        "        return build(\"youtube\", \"v3\", credentials=creds)\n",
        "    except HttpError as e:\n",
        "        print(f\"âŒ YouTube API error: {e}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Authentication failed: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def preflight_youtube():\n",
        "    \"\"\"\n",
        "    Confirm that we can load and refresh YouTube credentials.\n",
        "    We DON'T call any specific API endpoint that requires extra scopes,\n",
        "    because our token only guarantees youtube.upload.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Testing YouTube authentication...\")\n",
        "    yt = get_authenticated_service()\n",
        "    # If we got here without exception, credentials & scope are fine for uploads\n",
        "    print(\"âœ… YouTube credentials OK â€“ ready for uploads.\")\n",
        "    return yt\n",
        "\n",
        "\n",
        "# â”€â”€ ffmpeg preflight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def preflight_ffmpeg():\n",
        "    \"\"\"\n",
        "    Ensure ffmpeg is installed and callable.\n",
        "    Colab usually has ffmpeg preinstalled, but we confirm.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Testing ffmpeg availability...\")\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ffmpeg\", \"-version\"],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True,\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            first_line = result.stdout.splitlines()[0] if result.stdout else \"ffmpeg available\"\n",
        "            print(f\"âœ… ffmpeg OK â€“ {first_line}\")\n",
        "        else:\n",
        "            print(\"âŒ ffmpeg returned a non-zero exit code.\")\n",
        "            print(result.stderr[:500])\n",
        "            raise RuntimeError(\"ffmpeg not working properly.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"âŒ ffmpeg not found on PATH.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(\"âŒ ffmpeg preflight failed.\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# â”€â”€ Run all preflights â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print(\"ğŸš€ Running preflight checks...\\n\")\n",
        "\n",
        "preflight_openai(openai_client)\n",
        "preflight_suno(config)\n",
        "youtube_client = preflight_youtube()\n",
        "preflight_ffmpeg()\n",
        "\n",
        "print(\"\\nâœ… All preflight checks passed. Safe to proceed with the rest of the pipeline.\")\n"
      ],
      "metadata": {
        "id": "4-VDhBb9WiFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3. Topic / Persona / Tone / Format selection + run metadata\n",
        "#    - Multi-layer topic engine (families + seasonal + trending + rotation)\n",
        "#    - Content diversity engine via \"formats\" (maps to persona + tone)\n",
        "#    - Logs WHY a subject was chosen (seed vs trending vs mixed)\n",
        "#    - UPDATED trending: multi-day pool + random sampling + \"only if relevant\"\n",
        "#    - NEW: Soft title-length guidance for Suno (<= 80 chars) via GPT rewrite (no trimming)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import random\n",
        "import uuid\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import pathlib\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "\n",
        "# We reuse:\n",
        "# - openai_client (from Block 1)\n",
        "\n",
        "BASE_OUT_DIR = pathlib.Path(\"outputs\")\n",
        "BASE_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RECENT_SUBJECT_WINDOW = 50\n",
        "SEASONAL_FAMILY_BIAS_PROB = 0.7\n",
        "TOPIC_MODEL = \"gpt-4.1-mini\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Shorts-friendly length targets (for Block 4 lyric generation)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "SHORTS_TARGET_SECONDS = 120       # ideal target ~2:00\n",
        "SHORTS_SOFT_MAX_SECONDS = 175     # practical \"try to stay under\" (~2:55)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Suno title constraint (soft guidance; NO hard trimming)\n",
        "# Suno error: \"music title cannot exceed 80 characters\"\n",
        "# We'll guide GPT to stay under this, and do a gentle rewrite if it doesn't.\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "SUNO_TITLE_MAX_CHARS = 80\n",
        "SUNO_TITLE_SOFT_MAX_CHARS = 70   # aim below hard cap to reduce edge-case failures\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Subject families with more general-knowledge, view-friendly seeds\n",
        "# (Your preferred list; songs still explain deeply downstream)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "SUBJECT_FAMILIES = {\n",
        "    \"science_physics_space\": [\n",
        "        \"black holes\",\n",
        "        \"gravity in everyday life\",\n",
        "        \"relativity explained simply\",\n",
        "        \"why the sky is blue\",\n",
        "        \"how rockets escape Earth\",\n",
        "        \"time dilation in simple words\",\n",
        "        \"what dark matter might be\",\n",
        "        \"how GPS knows your location\",\n",
        "        \"why orbits don't fall down\",\n",
        "    ],\n",
        "    \"science_energy_earth\": [\n",
        "        \"how solar panels work\",\n",
        "        \"how batteries store energy\",\n",
        "        \"the greenhouse effect\",\n",
        "        \"nuclear fusion vs fission\",\n",
        "        \"how wind turbines generate power\",\n",
        "        \"the water cycle\",\n",
        "        \"climate change basics\",\n",
        "        \"why seasons exist\",\n",
        "        \"how electricity gets to your home\",\n",
        "    ],\n",
        "    \"science_bio_body\": [\n",
        "        \"evolution by natural selection\",\n",
        "        \"photosynthesis\",\n",
        "        \"how vaccines work\",\n",
        "        \"the human immune system basics\",\n",
        "        \"how neurons send signals\",\n",
        "        \"sleep cycles and circadian rhythm\",\n",
        "        \"how digestion works\",\n",
        "        \"how your heart pumps blood\",\n",
        "    ],\n",
        "    \"finance_and_economics\": [\n",
        "        \"inflation\",\n",
        "        \"compound interest\",\n",
        "        \"tax brackets\",\n",
        "        \"budgeting for beginners\",\n",
        "        \"what a stock is\",\n",
        "        \"what ETFs are\",\n",
        "        \"index funds vs stock picking\",\n",
        "        \"how mortgages work\",\n",
        "        \"good debt vs bad debt\",\n",
        "        \"credit scores explained\",\n",
        "        \"what GDP actually measures\",\n",
        "        \"how central banks influence interest rates\",\n",
        "        \"how government debt works\",\n",
        "    ],\n",
        "    \"history_and_society\": [\n",
        "        \"the fall of the Roman Empire\",\n",
        "        \"World War 2 in Europe (very high-level)\",\n",
        "        \"the French Revolution\",\n",
        "        \"the industrial revolution\",\n",
        "        \"the invention of the internet\",\n",
        "        \"the printing press and its impact\",\n",
        "        \"the Cold War in simple terms\",\n",
        "        \"the rise of social media\",\n",
        "        \"the history of money\",\n",
        "        \"how democracies develop over time\",\n",
        "    ],\n",
        "    \"psychology_and_life_skills\": [\n",
        "        \"procrastination\",\n",
        "        \"habits and habit loops\",\n",
        "        \"impostor syndrome\",\n",
        "        \"growth mindset\",\n",
        "        \"how memory works\",\n",
        "        \"decision fatigue\",\n",
        "        \"dopamine and motivation\",\n",
        "        \"introvert vs extrovert myths\",\n",
        "        \"cognitive biases in everyday life\",\n",
        "        \"how to build discipline\",\n",
        "        \"why we self-sabotage\",\n",
        "    ],\n",
        "    \"tech_and_digital_life\": [\n",
        "        \"how the internet works\",\n",
        "        \"what algorithms are\",\n",
        "        \"how recommendation systems work\",\n",
        "        \"what a blockchain is\",\n",
        "        \"password security basics\",\n",
        "        \"cloud computing explained simply\",\n",
        "        \"artificial intelligence vs machine learning\",\n",
        "        \"how encryption keeps data safe\",\n",
        "        \"how social media hooks your brain\",\n",
        "    ],\n",
        "    \"internet_and_creator_economy\": [\n",
        "        \"how the YouTube algorithm picks your next video\",\n",
        "        \"what watch time actually means for creators\",\n",
        "        \"why Shorts go viral but long videos don't\",\n",
        "        \"how TikTok decides what you see\",\n",
        "        \"what engagement really is on social media\",\n",
        "        \"how influencers actually make money\",\n",
        "        \"what brand deals and sponsorships really are\",\n",
        "        \"why some channels blow up overnight\",\n",
        "        \"what a content niche is and why it matters\",\n",
        "        \"how click-through rate affects your views\",\n",
        "    ],\n",
        "    \"gaming_and_virtual_worlds\": [\n",
        "        \"how Minecraft redstone is basically real circuits\",\n",
        "        \"why Minecraft is perfect for learning logic\",\n",
        "        \"what game XP teaches you about habits\",\n",
        "        \"how matchmaking and skill rating work in online games\",\n",
        "        \"loot boxes vs gambling in games\",\n",
        "        \"why Roblox is a whole economy not just a game\",\n",
        "        \"game currencies vs real money\",\n",
        "        \"how anti-cheat systems try to catch hackers\",\n",
        "        \"ping, lag and servers explained\",\n",
        "        \"what game design tricks keep you playing\",\n",
        "    ],\n",
        "    \"ai_and_future_of_work\": [\n",
        "        \"how AI chatbots actually work in simple terms\",\n",
        "        \"why AI hallucinates wrong answers\",\n",
        "        \"what training data really is\",\n",
        "        \"how image generators turn text into art\",\n",
        "        \"will AI take all jobs or just some tasks\",\n",
        "        \"what automation really means for your future work\",\n",
        "        \"how AI filters your social media feed\",\n",
        "        \"deepfakes explained in simple words\",\n",
        "        \"how to spot AI-generated content online\",\n",
        "        \"why AI still needs human supervision\",\n",
        "    ],\n",
        "    \"modern_money_and_side_hustles\": [\n",
        "        \"how affiliate marketing works in 60 seconds\",\n",
        "        \"dropshipping explained without the fake guru BS\",\n",
        "        \"what passive income really is and isn't\",\n",
        "        \"how taxes work when you make money online\",\n",
        "        \"why get rich quick is almost always a scam\",\n",
        "        \"how to budget your first paycheck\",\n",
        "        \"difference between salary hourly and freelance work\",\n",
        "        \"why investing beats saving over time\",\n",
        "        \"side hustles that are actually skills not schemes\",\n",
        "        \"how creators sell digital products\",\n",
        "    ],\n",
        "    \"digital_safety_and_privacy\": [\n",
        "        \"why password managers exist\",\n",
        "        \"how hackers guess your passwords\",\n",
        "        \"what phishing emails look like\",\n",
        "        \"why public wifi isn't really safe\",\n",
        "        \"what a VPN actually does\",\n",
        "        \"how end to end encryption keeps chats private\",\n",
        "        \"what cookies and tracking really are\",\n",
        "        \"how apps sell your data in plain language\",\n",
        "        \"what two factor authentication protects you from\",\n",
        "        \"how to spot a scam website\",\n",
        "    ],\n",
        "    \"world_systems_and_geopolitics\": [\n",
        "        \"how elections work in a democracy\",\n",
        "        \"what the United Nations actually does\",\n",
        "        \"what NATO is and why it was created\",\n",
        "        \"how sanctions between countries work\",\n",
        "        \"why interest rate decisions affect the entire world\",\n",
        "        \"what a trade war actually is\",\n",
        "        \"how oil prices affect your groceries\",\n",
        "        \"what the World Bank and IMF do\",\n",
        "        \"why global supply chains are so fragile\",\n",
        "        \"how borders between countries are decided\",\n",
        "    ],\n",
        "    \"internet_culture_and_psychology\": [\n",
        "        \"why memes spread like viruses\",\n",
        "        \"what doomscrolling does to your brain\",\n",
        "        \"why you can't stop checking notifications\",\n",
        "        \"FOMO explained in one minute\",\n",
        "        \"why comment sections are so toxic\",\n",
        "        \"paradox of choice and too many options\",\n",
        "        \"why outrage content gets more views\",\n",
        "        \"what parasocial relationships really are\",\n",
        "        \"why you feel like everyone else is doing better\",\n",
        "        \"how echo chambers can change your views\",\n",
        "    ],\n",
        "    \"learning_and_school_survival\": [\n",
        "        \"how memory actually works and how to hack it\",\n",
        "        \"spaced repetition explained with a song\",\n",
        "        \"why cramming the night before doesn't work\",\n",
        "        \"how to take notes so you actually remember\",\n",
        "        \"what focus really is and why you lose it\",\n",
        "        \"how to beat procrastination on homework\",\n",
        "        \"difference between studying hard and studying smart\",\n",
        "        \"how to use the Pomodoro technique for school\",\n",
        "        \"why multitasking kills your grades\",\n",
        "        \"what growth mindset means for exams\",\n",
        "    ],\n",
        "    \"everyday_science_and_tech\": [\n",
        "        \"how your phone battery actually works\",\n",
        "        \"why your phone overheats when gaming\",\n",
        "        \"how noise cancelling headphones trick your ears\",\n",
        "        \"why airplanes stay up in the sky\",\n",
        "        \"why your shower water changes temperature suddenly\",\n",
        "        \"how wifi sends data through the air\",\n",
        "        \"why microwaves heat food from inside\",\n",
        "        \"why some ice cubes are cloudy\",\n",
        "        \"how credit card tap to pay works\",\n",
        "        \"how GPS knows where you are\",\n",
        "    ],\n",
        "    \"myths_and_misconceptions\": [\n",
        "        \"no you don't use only ten percent of your brain\",\n",
        "        \"is sugar really as addictive as drugs\",\n",
        "        \"do phone chargers drain power when not in use\",\n",
        "        \"does detox actually mean anything\",\n",
        "        \"is multitasking a real skill\",\n",
        "        \"will cracking your knuckles give you arthritis\",\n",
        "        \"does eating late make you fat\",\n",
        "        \"is eight hours of sleep the only correct amount\",\n",
        "        \"are left brained vs right brained people real\",\n",
        "        \"do you really need ten thousand steps a day\",\n",
        "    ],\n",
        "    \"sports_and_performance_science\": [\n",
        "        \"what expected goals xG means in football\",\n",
        "        \"how player ratings are calculated in games like FIFA\",\n",
        "        \"why some players choke under pressure\",\n",
        "        \"how sprint speed and acceleration really work\",\n",
        "        \"why stamina wins more matches than talent\",\n",
        "        \"what sports analytics teams do behind the scenes\",\n",
        "        \"how reaction time can be trained\",\n",
        "        \"why home advantage exists in sports\",\n",
        "        \"how strategy changes in the final minutes of a game\",\n",
        "        \"how injuries actually happen in sports\",\n",
        "    ],\n",
        "    \"climate_and_future_cities\": [\n",
        "        \"how carbon footprints are actually calculated\",\n",
        "        \"what net zero really means\",\n",
        "        \"are electric cars really better for the planet\",\n",
        "        \"how smart cities use data to run everything\",\n",
        "        \"what a carbon tax is\",\n",
        "        \"how heat pumps warm your house efficiently\",\n",
        "        \"why cities are getting hotter over time\",\n",
        "        \"how public transport systems are designed\",\n",
        "        \"how renewable energy fits into the grid\",\n",
        "        \"what climate tipping points are\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Format engine\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "FORMAT_DEFS = {\n",
        "    \"explain_like_im_5\": {\n",
        "        \"label\": \"Explain Like I'm 5\",\n",
        "        \"persona\": \"lofi chillhop, soft and cozy\",\n",
        "        \"tone\": \"wholesome and uplifting\",\n",
        "        \"background_style\": \"simple cute illustration, soft pastel colors, rounded shapes, cozy classroom vibe\",\n",
        "    },\n",
        "    \"epic_breakdown\": {\n",
        "        \"label\": \"Epic Breakdown\",\n",
        "        \"persona\": \"epic cinematic soundtrack, big drums\",\n",
        "        \"tone\": \"dramatic and intense\",\n",
        "        \"background_style\": \"cinematic wide shot, dramatic lighting, epic atmosphere\",\n",
        "    },\n",
        "    \"science_pop_song\": {\n",
        "        \"label\": \"Science Pop Song\",\n",
        "        \"persona\": \"upbeat pop-punk, high energy guitars\",\n",
        "        \"tone\": \"motivational and energetic\",\n",
        "        \"background_style\": \"bright bold colors, energetic composition, playful science icons\",\n",
        "    },\n",
        "    \"rap_summary\": {\n",
        "        \"label\": \"Rap Summary\",\n",
        "        \"persona\": \"dark trap, heavy 808s, moody\",\n",
        "        \"tone\": \"sarcastic but friendly\",\n",
        "        \"background_style\": \"urban neon style, bold contrast, dynamic graffiti-like shapes\",\n",
        "    },\n",
        "    \"history_musical\": {\n",
        "        \"label\": \"History Musical\",\n",
        "        \"persona\": \"Disney-style musical, orchestral pop\",\n",
        "        \"tone\": \"thoughtful and reflective\",\n",
        "        \"background_style\": \"painterly historical scenes, warm lighting, storytelling composition\",\n",
        "    },\n",
        "    \"mindset_diary\": {\n",
        "        \"label\": \"Mindset Diary\",\n",
        "        \"persona\": \"indie folk, acoustic and warm\",\n",
        "        \"tone\": \"calm and reassuring\",\n",
        "        \"background_style\": \"soft vignette, cozy room, journal-style illustration\",\n",
        "    },\n",
        "    \"techwave\": {\n",
        "        \"label\": \"Techwave\",\n",
        "        \"persona\": \"synthwave, nostalgic 80s electronic\",\n",
        "        \"tone\": \"mysterious and slightly spooky\",\n",
        "        \"background_style\": \"neon grids, abstract tech shapes, glowing circuitry\",\n",
        "    },\n",
        "}\n",
        "\n",
        "FAMILY_FORMAT_MAP = {\n",
        "    \"science_physics_space\": [\"explain_like_im_5\", \"epic_breakdown\", \"science_pop_song\", \"techwave\"],\n",
        "    \"science_energy_earth\": [\"explain_like_im_5\", \"science_pop_song\", \"epic_breakdown\"],\n",
        "    \"science_bio_body\": [\"explain_like_im_5\", \"science_pop_song\", \"mindset_diary\"],\n",
        "    \"finance_and_economics\": [\"rap_summary\", \"explain_like_im_5\", \"mindset_diary\"],\n",
        "    \"history_and_society\": [\"history_musical\", \"epic_breakdown\", \"explain_like_im_5\"],\n",
        "    \"psychology_and_life_skills\": [\"mindset_diary\", \"explain_like_im_5\", \"rap_summary\"],\n",
        "    \"tech_and_digital_life\": [\"techwave\", \"science_pop_song\", \"explain_like_im_5\"],\n",
        "\n",
        "    \"internet_and_creator_economy\": [\"rap_summary\", \"techwave\", \"explain_like_im_5\"],\n",
        "    \"gaming_and_virtual_worlds\": [\"techwave\", \"science_pop_song\", \"rap_summary\"],\n",
        "    \"ai_and_future_of_work\": [\"techwave\", \"science_pop_song\", \"epic_breakdown\"],\n",
        "    \"modern_money_and_side_hustles\": [\"rap_summary\", \"explain_like_im_5\", \"mindset_diary\"],\n",
        "    \"digital_safety_and_privacy\": [\"explain_like_im_5\", \"epic_breakdown\", \"techwave\"],\n",
        "    \"world_systems_and_geopolitics\": [\"history_musical\", \"epic_breakdown\", \"explain_like_im_5\"],\n",
        "    \"internet_culture_and_psychology\": [\"rap_summary\", \"mindset_diary\", \"techwave\"],\n",
        "    \"learning_and_school_survival\": [\"explain_like_im_5\", \"mindset_diary\", \"science_pop_song\"],\n",
        "    \"everyday_science_and_tech\": [\"explain_like_im_5\", \"science_pop_song\", \"techwave\"],\n",
        "    \"myths_and_misconceptions\": [\"explain_like_im_5\", \"rap_summary\", \"science_pop_song\"],\n",
        "    \"sports_and_performance_science\": [\"science_pop_song\", \"epic_breakdown\", \"mindset_diary\"],\n",
        "    \"climate_and_future_cities\": [\"science_pop_song\", \"epic_breakdown\", \"explain_like_im_5\"],\n",
        "}\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Seasonal profiles\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def get_seasonal_profile(now: datetime) -> dict:\n",
        "    m = now.month\n",
        "    if m == 1:\n",
        "        return {\n",
        "            \"label\": \"New Year: money, habits & fresh start\",\n",
        "            \"preferred_families\": [\n",
        "                \"finance_and_economics\",\n",
        "                \"modern_money_and_side_hustles\",\n",
        "                \"psychology_and_life_skills\",\n",
        "                \"learning_and_school_survival\",\n",
        "            ],\n",
        "            \"notes\": \"New year goals, money clean-up, habit building, self-improvement.\"\n",
        "        }\n",
        "    elif m == 2:\n",
        "        return {\n",
        "            \"label\": \"Relationships, mind & digital life\",\n",
        "            \"preferred_families\": [\n",
        "                \"psychology_and_life_skills\",\n",
        "                \"science_bio_body\",\n",
        "                \"internet_culture_and_psychology\",\n",
        "            ],\n",
        "            \"notes\": \"Emotions, social life, online relationships, how our body/brain works.\"\n",
        "        }\n",
        "    elif m in (3, 4):\n",
        "        return {\n",
        "            \"label\": \"Spring: energy, nature & environment\",\n",
        "            \"preferred_families\": [\n",
        "                \"science_energy_earth\",\n",
        "                \"science_bio_body\",\n",
        "                \"climate_and_future_cities\",\n",
        "                \"everyday_science_and_tech\",\n",
        "            ],\n",
        "            \"notes\": \"Energy, climate, biology waking up, environment and cities.\"\n",
        "        }\n",
        "    elif m in (5, 6):\n",
        "        return {\n",
        "            \"label\": \"End of school year: history & big ideas\",\n",
        "            \"preferred_families\": [\n",
        "                \"history_and_society\",\n",
        "                \"science_physics_space\",\n",
        "                \"world_systems_and_geopolitics\",\n",
        "            ],\n",
        "            \"notes\": \"History recaps, big systems, how the world fits together.\"\n",
        "        }\n",
        "    elif m in (7, 8):\n",
        "        return {\n",
        "            \"label\": \"Summer curiosity: games, world & tech\",\n",
        "            \"preferred_families\": [\n",
        "                \"science_energy_earth\",\n",
        "                \"tech_and_digital_life\",\n",
        "                \"gaming_and_virtual_worlds\",\n",
        "                \"internet_and_creator_economy\",\n",
        "            ],\n",
        "            \"notes\": \"Weird nature, gaming, creators, tech deep-dives while people have free time.\"\n",
        "        }\n",
        "    elif m == 9:\n",
        "        return {\n",
        "            \"label\": \"Back to school: basics & survival\",\n",
        "            \"preferred_families\": [\n",
        "                \"science_physics_space\",\n",
        "                \"finance_and_economics\",\n",
        "                \"psychology_and_life_skills\",\n",
        "                \"learning_and_school_survival\",\n",
        "                \"everyday_science_and_tech\",\n",
        "            ],\n",
        "            \"notes\": \"Core concepts for school and life, study hacks and practical basics.\"\n",
        "        }\n",
        "    elif m in (10,):\n",
        "        return {\n",
        "            \"label\": \"Spooky science, mind & myths\",\n",
        "            \"preferred_families\": [\n",
        "                \"science_physics_space\",\n",
        "                \"psychology_and_life_skills\",\n",
        "                \"myths_and_misconceptions\",\n",
        "                \"internet_culture_and_psychology\",\n",
        "            ],\n",
        "            \"notes\": \"Slightly spooky science, mind, and busting viral myths.\"\n",
        "        }\n",
        "    elif m in (11,):\n",
        "        return {\n",
        "            \"label\": \"Money, systems & the world\",\n",
        "            \"preferred_families\": [\n",
        "                \"finance_and_economics\",\n",
        "                \"modern_money_and_side_hustles\",\n",
        "                \"history_and_society\",\n",
        "                \"world_systems_and_geopolitics\",\n",
        "                \"ai_and_future_of_work\",\n",
        "            ],\n",
        "            \"notes\": \"Economy, world systems, future of work before year-end.\"\n",
        "        }\n",
        "    else:  # December\n",
        "        return {\n",
        "            \"label\": \"Year-in-review, world & future\",\n",
        "            \"preferred_families\": [\n",
        "                \"history_and_society\",\n",
        "                \"tech_and_digital_life\",\n",
        "                \"science_physics_space\",\n",
        "                \"world_systems_and_geopolitics\",\n",
        "                \"internet_and_creator_economy\",\n",
        "                \"ai_and_future_of_work\",\n",
        "            ],\n",
        "            \"notes\": \"Inventions, big events, creators, AI and where the world is heading.\"\n",
        "        }\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# History: recent subjects\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def load_recent_subjects(base_dir: pathlib.Path, window: int = RECENT_SUBJECT_WINDOW) -> list[str]:\n",
        "    if not base_dir.exists():\n",
        "        return []\n",
        "    meta_files = sorted(\n",
        "        base_dir.glob(\"run_*/run_meta.json\"),\n",
        "        key=lambda p: p.stat().st_mtime,\n",
        "        reverse=True,\n",
        "    )\n",
        "    recent_subjects = []\n",
        "    for mf in meta_files[:window]:\n",
        "        try:\n",
        "            data = json.loads(mf.read_text(encoding=\"utf-8\"))\n",
        "            subj = data.get(\"subject\")\n",
        "            if subj:\n",
        "                recent_subjects.append(subj)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return recent_subjects\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Trending pulse (UPDATED)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "TREND_DAYS_LOOKBACK = 10           # scan last 10 days\n",
        "TREND_POOL_MAX_ITEMS = 30          # final deduped pool size\n",
        "TREND_GPT_SAMPLE_K = 7             # how many keywords we pass into GPT each run\n",
        "TREND_MIN_USE_PROB = 0.55          # even if we have trends, only include them ~55% of the time\n",
        "\n",
        "def fetch_trending_keywords(\n",
        "    max_items: int = TREND_POOL_MAX_ITEMS,\n",
        "    days_lookback: int = TREND_DAYS_LOOKBACK,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      {\n",
        "        \"pool\": [kw1, kw2, ...],        # deduped pool (size <= max_items)\n",
        "        \"sample_for_gpt\": [...],        # random subset for this run\n",
        "        \"source\": \"wikipedia_pageviews\",\n",
        "        \"days_used\": [date_strs...],\n",
        "        \"errors\": [ ... ],\n",
        "      }\n",
        "    \"\"\"\n",
        "    def _fetch_for_date(d):\n",
        "        url = (\n",
        "            \"https://wikimedia.org/api/rest_v1/metrics/pageviews/top/\"\n",
        "            f\"en.wikipedia/all-access/{d.year}/{d.month:02d}/{d.day:02d}\"\n",
        "        )\n",
        "        headers = {\n",
        "            \"User-Agent\": \"suno-shorts-bot/1.0 (contact: your-email@example.com)\"\n",
        "        }\n",
        "        r = requests.get(url, headers=headers, timeout=12)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        items = data.get(\"items\", [])\n",
        "        if not items:\n",
        "            return []\n",
        "        articles = items[0].get(\"articles\", [])\n",
        "        out = []\n",
        "        for art in articles:\n",
        "            title = art.get(\"article\", \"\")\n",
        "            if not title:\n",
        "                continue\n",
        "            if title in (\"Main_Page\", \"Special:Search\"):\n",
        "                continue\n",
        "            if \":\" in title:\n",
        "                continue\n",
        "\n",
        "            clean = title.replace(\"_\", \" \").strip()\n",
        "\n",
        "            if len(clean) < 3 or len(clean) > 45:\n",
        "                continue\n",
        "            if re.fullmatch(r\"\\d+\", clean):\n",
        "                continue\n",
        "            if re.search(r\"[<>@#%{}|\\\\^~\\[\\]]\", clean):\n",
        "                continue\n",
        "\n",
        "            out.append(clean)\n",
        "\n",
        "        return out\n",
        "\n",
        "    today = datetime.now(timezone.utc).date()\n",
        "\n",
        "    errors = []\n",
        "    used_days = []\n",
        "    collected = []\n",
        "\n",
        "    for delta in range(2, 2 + days_lookback):\n",
        "        d = today - timedelta(days=delta)\n",
        "        try:\n",
        "            kws = _fetch_for_date(d)\n",
        "            if kws:\n",
        "                used_days.append(str(d))\n",
        "                collected.extend(kws)\n",
        "        except Exception as e:\n",
        "            errors.append(f\"{d}: {e}\")\n",
        "\n",
        "    seen = set()\n",
        "    deduped = []\n",
        "    for kw in collected:\n",
        "        k = kw.strip()\n",
        "        kl = k.lower()\n",
        "        if kl in seen:\n",
        "            continue\n",
        "        seen.add(kl)\n",
        "        deduped.append(k)\n",
        "\n",
        "    soft_downweight_terms = {\n",
        "        \"ai\", \"artificial intelligence\", \"chatgpt\", \"openai\",\n",
        "        \"machine learning\", \"deep learning\", \"llm\", \"gpt\"\n",
        "    }\n",
        "\n",
        "    aiish = []\n",
        "    non_aiish = []\n",
        "    for kw in deduped:\n",
        "        low = kw.lower()\n",
        "        if any(t in low for t in soft_downweight_terms):\n",
        "            aiish.append(kw)\n",
        "        else:\n",
        "            non_aiish.append(kw)\n",
        "\n",
        "    final_pool = []\n",
        "    final_pool.extend(non_aiish[:max_items])\n",
        "    remaining = max(0, max_items - len(final_pool))\n",
        "    if remaining > 0:\n",
        "        final_pool.extend(aiish[: min(3, remaining)])\n",
        "\n",
        "    if len(final_pool) < 10:\n",
        "        remaining = max(0, max_items - len(final_pool))\n",
        "        final_pool.extend(aiish[:remaining])\n",
        "\n",
        "    if final_pool:\n",
        "        k = min(TREND_GPT_SAMPLE_K, len(final_pool))\n",
        "        sample_for_gpt = random.sample(final_pool, k=k)\n",
        "    else:\n",
        "        sample_for_gpt = []\n",
        "\n",
        "    return {\n",
        "        \"pool\": final_pool[:max_items],\n",
        "        \"sample_for_gpt\": sample_for_gpt,\n",
        "        \"source\": \"wikipedia_pageviews\",\n",
        "        \"days_used\": used_days,\n",
        "        \"errors\": errors,\n",
        "    }\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# GPT topic generation (UPDATED: \"only if relevant\" + title length guidance)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def gpt_generate_topic_from_context(\n",
        "    family_name: str,\n",
        "    base_ideas: list[str],\n",
        "    recent_subjects: list[str],\n",
        "    seasonal_profile: dict,\n",
        "    trending_keywords: list[str],\n",
        "    language: str = \"English\",\n",
        ") -> str:\n",
        "    recent_snippet = \"\"\n",
        "    if recent_subjects:\n",
        "        recent_snippet = \"\\nRecently used topics (avoid repeating these exactly):\\n- \" + \"\\n- \".join(\n",
        "            recent_subjects[:20]\n",
        "        )\n",
        "\n",
        "    seasonal_label = seasonal_profile.get(\"label\", \"General\")\n",
        "    seasonal_fams = seasonal_profile.get(\"preferred_families\", [])\n",
        "    seasonal_notes = seasonal_profile.get(\"notes\", \"\")\n",
        "\n",
        "    trending_snippet = \"\"\n",
        "    if trending_keywords:\n",
        "        trending_snippet = (\n",
        "            \"\\nTrending keywords (random sample). Use them ONLY if genuinely relevant to this family.\\n\"\n",
        "            \"If none are relevant, IGNORE them completely.\\n\"\n",
        "            \"- \" + \"\\n- \".join(trending_keywords)\n",
        "        )\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are helping design topics for educational YouTube Shorts songs in {language}.\\n\\n\"\n",
        "        f\"Subject family: {family_name}\\n\"\n",
        "        \"Base seed ideas (this is your main source):\\n\"\n",
        "        + \"\\n\".join(f\"- {idea}\" for idea in base_ideas)\n",
        "        + \"\\n\"\n",
        "        + recent_snippet\n",
        "        + \"\\n\\nSeasonal context right now:\\n\"\n",
        "        f\"- Theme label: {seasonal_label}\\n\"\n",
        "        f\"- Seasonal notes: {seasonal_notes}\\n\"\n",
        "        f\"- Seasonal preferred families: {', '.join(seasonal_fams)}\\n\"\n",
        "        + (trending_snippet if trending_keywords else \"\\n(No trending keywords available for this run.)\")\n",
        "        + \"\\n\\nRules for using trends:\\n\"\n",
        "        \"- First pick a strong topic based on the BASE SEEDS.\\n\"\n",
        "        \"- Then check the trending keywords: if ONE is clearly relevant, you may gently adapt the topic to connect.\\n\"\n",
        "        \"- Do NOT pivot the whole topic to something unrelated.\\n\"\n",
        "        \"- If nothing is relevant, ignore trends.\\n\\n\"\n",
        "        \"Your task:\\n\"\n",
        "        \"- Output ONE specific, engaging topic title for an educational song.\\n\"\n",
        "        \"- Short (3â€“8 words), clear, beginner-friendly.\\n\"\n",
        "        \"- Safe for a general audience.\\n\"\n",
        "        \"- Politics/geopolitics allowed ONLY neutrally; no endorsements.\\n\"\n",
        "        \"- Do NOT repeat any recently used topic exactly.\\n\"\n",
        "        f\"- IMPORTANT: This will be used as a music TITLE. Aim <= {SUNO_TITLE_SOFT_MAX_CHARS} characters.\\n\"\n",
        "        \"- No quotes, emojis, or explanations.\\n\"\n",
        "        \"- Return ONLY the topic line.\\n\"\n",
        "    )\n",
        "\n",
        "    resp = openai_client.chat.completions.create(\n",
        "        model=TOPIC_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=64,\n",
        "    )\n",
        "\n",
        "    topic = resp.choices[0].message.content.strip()\n",
        "\n",
        "    if topic.startswith('\"') and topic.endswith('\"'):\n",
        "        topic = topic[1:-1].strip()\n",
        "    if topic.startswith(\"'\") and topic.endswith(\"'\"):\n",
        "        topic = topic[1:-1].strip()\n",
        "\n",
        "    if len(topic.split()) > 10:\n",
        "        topic = \" \".join(topic.split()[:10])\n",
        "\n",
        "    return topic\n",
        "\n",
        "def gpt_rewrite_short_title(\n",
        "    topic: str,\n",
        "    family_name: str,\n",
        "    base_ideas: list[str],\n",
        "    trending_keywords: list[str],\n",
        "    language: str = \"English\",\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    If GPT returns a title that's too long for Suno, ask GPT to rewrite it shorter.\n",
        "    This is NOT trimming; it's regeneration with constraints.\n",
        "    \"\"\"\n",
        "    seeds = \"\\n\".join(f\"- {i}\" for i in base_ideas[:6])\n",
        "    trends = \"\\n\".join(f\"- {k}\" for k in trending_keywords[:7]) if trending_keywords else \"(none)\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"Rewrite this music title to be SHORTER for YouTube + music platforms.\\n\\n\"\n",
        "        f\"Original title:\\n{topic}\\n\\n\"\n",
        "        f\"Context:\\n- Family: {family_name}\\n- Seed ideas:\\n{seeds}\\n- Trending keywords (optional):\\n{trends}\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- Keep the SAME meaning, just shorter and cleaner.\\n\"\n",
        "        \"- Stay beginner-friendly, but still specific.\\n\"\n",
        "        \"- No hype, no emojis, no quotes.\\n\"\n",
        "        f\"- HARD REQUIREMENT: <= {SUNO_TITLE_MAX_CHARS} characters.\\n\"\n",
        "        f\"- Aim <= {SUNO_TITLE_SOFT_MAX_CHARS} characters if possible.\\n\"\n",
        "        \"- Output ONLY the rewritten title line.\\n\"\n",
        "    )\n",
        "\n",
        "    resp = openai_client.chat.completions.create(\n",
        "        model=TOPIC_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.4,\n",
        "        max_tokens=64,\n",
        "    )\n",
        "    out = resp.choices[0].message.content.strip()\n",
        "\n",
        "    if out.startswith('\"') and out.endswith('\"'):\n",
        "        out = out[1:-1].strip()\n",
        "    if out.startswith(\"'\") and out.endswith(\"'\"):\n",
        "        out = out[1:-1].strip()\n",
        "\n",
        "    if len(out.split()) > 10:\n",
        "        out = \" \".join(out.split()[:10])\n",
        "\n",
        "    return out\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Topic origin classifier (seed vs trending vs mixed)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def classify_topic_origin(topic: str, base_ideas: list[str], trending_keywords: list[str]) -> tuple[str, dict]:\n",
        "    tl = topic.lower()\n",
        "\n",
        "    matched_trending = []\n",
        "    for kw in trending_keywords:\n",
        "        kl = kw.lower()\n",
        "        if kl in tl or tl in kl:\n",
        "            matched_trending.append(kw)\n",
        "\n",
        "    matched_seeds = []\n",
        "    for idea in base_ideas:\n",
        "        il = idea.lower()\n",
        "        if il in tl or tl in il:\n",
        "            matched_seeds.append(idea)\n",
        "\n",
        "    if matched_trending and not matched_seeds:\n",
        "        origin = \"trending_inspired\"\n",
        "    elif matched_seeds and not matched_trending:\n",
        "        origin = \"seed_based\"\n",
        "    elif matched_seeds and matched_trending:\n",
        "        origin = \"mixed_seed_and_trending\"\n",
        "    else:\n",
        "        origin = \"mixed_or_derived\"\n",
        "\n",
        "    details = {\n",
        "        \"matched_trending_keywords\": matched_trending,\n",
        "        \"matched_seed_ideas\": matched_seeds,\n",
        "    }\n",
        "    return origin, details\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Format picker\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def pick_format_for_family(family_name: str):\n",
        "    formats = FAMILY_FORMAT_MAP.get(family_name) or list(FORMAT_DEFS.keys())\n",
        "    fmt_id = random.choice(formats)\n",
        "    fmt_cfg = FORMAT_DEFS[fmt_id]\n",
        "    return fmt_id, fmt_cfg\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Run directory helper\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def create_new_run_dir(base_dir: pathlib.Path = BASE_OUT_DIR) -> pathlib.Path:\n",
        "    timestamp = datetime.now(timezone.utc).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    short_id = uuid.uuid4().hex[:8]\n",
        "    run_name = f\"run_{timestamp}_{short_id}\"\n",
        "    run_dir = base_dir / run_name\n",
        "    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (run_dir / \"audio\").mkdir(exist_ok=True)\n",
        "    (run_dir / \"video\").mkdir(exist_ok=True)\n",
        "    (run_dir / \"images\").mkdir(exist_ok=True)\n",
        "    (run_dir / \"logs\").mkdir(exist_ok=True)\n",
        "    return run_dir\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Orchestrate\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def pick_subject_and_format(base_dir: pathlib.Path):\n",
        "    now = datetime.now(timezone.utc)\n",
        "    seasonal_profile = get_seasonal_profile(now)\n",
        "    recent_subjects = load_recent_subjects(base_dir, RECENT_SUBJECT_WINDOW)\n",
        "\n",
        "    # 1) Choose family (seasonal bias)\n",
        "    all_fams = list(SUBJECT_FAMILIES.keys())\n",
        "    seasonal_fams = [\n",
        "        fam for fam in seasonal_profile.get(\"preferred_families\", []) if fam in SUBJECT_FAMILIES\n",
        "    ]\n",
        "    if seasonal_fams and random.random() < SEASONAL_FAMILY_BIAS_PROB:\n",
        "        family_name = random.choice(seasonal_fams)\n",
        "        seasonal_choice = True\n",
        "    else:\n",
        "        family_name = random.choice(all_fams)\n",
        "        seasonal_choice = False\n",
        "\n",
        "    base_list = SUBJECT_FAMILIES[family_name]\n",
        "    num_seeds = min(5, len(base_list))\n",
        "    base_ideas = random.sample(base_list, k=num_seeds)\n",
        "\n",
        "    # 2) Trending: build pool from last N days, then provide random sample for GPT sometimes\n",
        "    trend_pack = fetch_trending_keywords(max_items=TREND_POOL_MAX_ITEMS, days_lookback=TREND_DAYS_LOOKBACK)\n",
        "    trend_pool = trend_pack[\"pool\"]\n",
        "    if trend_pool and (random.random() < TREND_MIN_USE_PROB):\n",
        "        trending_for_gpt = trend_pack[\"sample_for_gpt\"]\n",
        "        trending_used = True\n",
        "    else:\n",
        "        trending_for_gpt = []\n",
        "        trending_used = False\n",
        "\n",
        "    try:\n",
        "        topic = gpt_generate_topic_from_context(\n",
        "            family_name=family_name,\n",
        "            base_ideas=base_ideas,\n",
        "            recent_subjects=recent_subjects,\n",
        "            seasonal_profile=seasonal_profile,\n",
        "            trending_keywords=trending_for_gpt,\n",
        "            language=\"English\",\n",
        "        )\n",
        "\n",
        "        # If title too long for Suno, do ONE rewrite attempt (no trimming).\n",
        "        if len(topic) > SUNO_TITLE_MAX_CHARS:\n",
        "            print(f\"âš ï¸ Topic title too long for Suno ({len(topic)} chars). Rewriting shorter...\")\n",
        "            topic2 = gpt_rewrite_short_title(\n",
        "                topic=topic,\n",
        "                family_name=family_name,\n",
        "                base_ideas=base_ideas,\n",
        "                trending_keywords=trending_for_gpt,\n",
        "                language=\"English\",\n",
        "            )\n",
        "            print(f\"   â†’ Rewritten: {topic2} ({len(topic2)} chars)\")\n",
        "            topic = topic2\n",
        "\n",
        "        if topic in recent_subjects:\n",
        "            print(\"âš ï¸ GPT topic was in recent subjects, falling back to a base idea.\")\n",
        "            topic = random.choice(base_ideas)\n",
        "\n",
        "        topic_origin, origin_details = classify_topic_origin(topic, base_ideas, trending_for_gpt)\n",
        "\n",
        "        print(\n",
        "            f\"ğŸ¯ GPT-chosen topic: {topic} \"\n",
        "            f\"(family: {family_name}, seasonal_bias={seasonal_choice}, trending_in_prompt={trending_used})\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ GPT topic generation failed, falling back to random base idea:\", e)\n",
        "        topic = random.choice(base_list)\n",
        "        topic_origin = \"seed_based_fallback\"\n",
        "        origin_details = {\"matched_trending_keywords\": [], \"matched_seed_ideas\": [topic]}\n",
        "        print(f\"ğŸ¯ Fallback topic: {topic} (family: {family_name})\")\n",
        "\n",
        "    # 3) Format\n",
        "    format_id, fmt_cfg = pick_format_for_family(family_name)\n",
        "    persona = fmt_cfg[\"persona\"]\n",
        "    tone = fmt_cfg[\"tone\"]\n",
        "    format_label = fmt_cfg[\"label\"]\n",
        "    background_style_hint = fmt_cfg[\"background_style\"]\n",
        "\n",
        "    # Explanation text\n",
        "    seasonal_mode = \"seasonal family\" if seasonal_choice else \"non-seasonal random family\"\n",
        "    trend_desc = (\n",
        "        \"no trending pool available\"\n",
        "        if not trend_pool\n",
        "        else f\"trend pool size={len(trend_pool)} (lookback={TREND_DAYS_LOOKBACK}d)\"\n",
        "    )\n",
        "    trend_prompt_desc = (\n",
        "        \"trending NOT used in GPT prompt\"\n",
        "        if not trending_for_gpt\n",
        "        else f\"trending sample used in prompt={trending_for_gpt}\"\n",
        "    )\n",
        "\n",
        "    origin_text = (\n",
        "        f\"Family chosen via {seasonal_mode}. \"\n",
        "        f\"Base seeds sampled: {base_ideas}. \"\n",
        "        f\"{trend_desc}. {trend_prompt_desc}. \"\n",
        "        f\"Topic classified as '{topic_origin}'.\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"subject\": topic,\n",
        "        \"family_name\": family_name,\n",
        "        \"format_id\": format_id,\n",
        "        \"format_label\": format_label,\n",
        "        \"persona\": persona,\n",
        "        \"tone\": tone,\n",
        "        \"background_style_hint\": background_style_hint,\n",
        "        \"season_label\": seasonal_profile.get(\"label\", \"General\"),\n",
        "        \"seasonal_choice\": seasonal_choice,\n",
        "        # Trending debug\n",
        "        \"trend_source\": trend_pack.get(\"source\"),\n",
        "        \"trend_days_used\": trend_pack.get(\"days_used\", []),\n",
        "        \"trend_errors\": trend_pack.get(\"errors\", []),\n",
        "        \"trending_pool\": trend_pool[:15],\n",
        "        \"trending_used_in_prompt\": trending_used,\n",
        "        \"trending_prompt_sample\": trending_for_gpt,\n",
        "        # Topic selection info\n",
        "        \"base_ideas_used\": base_ideas,\n",
        "        \"topic_origin\": topic_origin,\n",
        "        \"topic_origin_details\": origin_details,\n",
        "        \"topic_selection_explanation\": origin_text,\n",
        "        # Title-length debug\n",
        "        \"suno_title_max_chars\": SUNO_TITLE_MAX_CHARS,\n",
        "        \"suno_title_soft_max_chars\": SUNO_TITLE_SOFT_MAX_CHARS,\n",
        "        \"subject_char_count\": len(topic),\n",
        "    }\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Create run + pick theme\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "RUN_DIR = create_new_run_dir()\n",
        "pick_result = pick_subject_and_format(BASE_OUT_DIR)\n",
        "\n",
        "subject = pick_result[\"subject\"]\n",
        "persona = pick_result[\"persona\"]\n",
        "tone = pick_result[\"tone\"]\n",
        "\n",
        "run_meta = {\n",
        "    \"run_dir\": str(RUN_DIR),\n",
        "    \"created_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "    \"subject\": subject,\n",
        "    \"topic_family\": pick_result[\"family_name\"],\n",
        "    \"format_id\": pick_result[\"format_id\"],\n",
        "    \"format_label\": pick_result[\"format_label\"],\n",
        "    \"persona\": persona,\n",
        "    \"tone\": tone,\n",
        "    \"season_label\": pick_result[\"season_label\"],\n",
        "    \"seasonal_choice\": pick_result[\"seasonal_choice\"],\n",
        "    \"background_style_hint\": pick_result[\"background_style_hint\"],\n",
        "\n",
        "    # Topic engine logs\n",
        "    \"base_ideas_used\": pick_result[\"base_ideas_used\"],\n",
        "    \"topic_origin\": pick_result[\"topic_origin\"],\n",
        "    \"topic_origin_details\": pick_result[\"topic_origin_details\"],\n",
        "    \"topic_selection_explanation\": pick_result[\"topic_selection_explanation\"],\n",
        "\n",
        "    # Trending logs\n",
        "    \"trend_source\": pick_result[\"trend_source\"],\n",
        "    \"trend_days_used\": pick_result[\"trend_days_used\"],\n",
        "    \"trend_errors\": pick_result[\"trend_errors\"],\n",
        "    \"trending_pool_preview\": pick_result[\"trending_pool\"],\n",
        "    \"trending_used_in_prompt\": pick_result[\"trending_used_in_prompt\"],\n",
        "    \"trending_prompt_sample\": pick_result[\"trending_prompt_sample\"],\n",
        "\n",
        "    # Title constraints for Suno\n",
        "    \"suno_title_max_chars\": pick_result.get(\"suno_title_max_chars\", SUNO_TITLE_MAX_CHARS),\n",
        "    \"suno_title_soft_max_chars\": pick_result.get(\"suno_title_soft_max_chars\", SUNO_TITLE_SOFT_MAX_CHARS),\n",
        "    \"subject_char_count\": pick_result.get(\"subject_char_count\", len(subject)),\n",
        "\n",
        "    # Length targets for Block 4 lyric generation\n",
        "    \"shorts_target_seconds\": SHORTS_TARGET_SECONDS,\n",
        "    \"shorts_soft_max_seconds\": SHORTS_SOFT_MAX_SECONDS,\n",
        "}\n",
        "\n",
        "meta_path = RUN_DIR / \"run_meta.json\"\n",
        "meta_path.write_text(json.dumps(run_meta, indent=2, ensure_ascii=False))\n",
        "\n",
        "print(\"ğŸ¬ New run initialized:\")\n",
        "print(\"  Run dir      :\", RUN_DIR)\n",
        "print(\"  Subject      :\", subject, f\"({len(subject)} chars)\")\n",
        "print(\"  Topic family :\", pick_result[\"family_name\"])\n",
        "print(\"  Format       :\", pick_result[\"format_label\"], f\"({pick_result['format_id']})\")\n",
        "print(\"  Persona      :\", persona)\n",
        "print(\"  Tone         :\", tone)\n",
        "print(\"  Season label :\", pick_result[\"season_label\"])\n",
        "print(\"  Topic origin :\", pick_result[\"topic_origin\"])\n",
        "print(\"  Explanation  :\", pick_result[\"topic_selection_explanation\"])\n",
        "print(\"  Target length:\", f\"~{SHORTS_TARGET_SECONDS}s (try stay under ~{SHORTS_SOFT_MAX_SECONDS}s)\")\n",
        "print(\"  Title limit  :\", f\"aim <= {SUNO_TITLE_SOFT_MAX_CHARS}, must be <= {SUNO_TITLE_MAX_CHARS}\")\n",
        "print(\"  Trend pool   :\", f\"{len(pick_result.get('trending_pool', []))} (preview stored in run_meta)\")\n",
        "print(\"  Trend prompt :\", \"USED\" if pick_result[\"trending_prompt_sample\"] else \"NOT used\")\n",
        "print(f\"  Metadata saved to: {meta_path}\")\n"
      ],
      "metadata": {
        "id": "Kuf-bk7WeXxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 4. GPT: teaching bullets â†’ dense, rhyming explainer lyrics\n",
        "#     (pace-aware word budgets to *guide* Shorts staying < ~3 minutes)\n",
        "#     UPDATED for: general-audience topics explained deeply (intuition-first)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "from typing import List\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "\n",
        "# Model for lyrics generation\n",
        "GPT_LYRICS_MODEL = \"gpt-4.1\"       # stronger for creative writing\n",
        "LYRICS_LANGUAGE = \"Portuguese (Brazil)\"        # e.g. \"English\" or \"Danish\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Load run metadata from Block 3 (run_meta.json)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "run_meta_path = RUN_DIR / \"run_meta.json\"\n",
        "if not run_meta_path.exists():\n",
        "    raise FileNotFoundError(f\"run_meta.json not found at {run_meta_path}. Run Block 3 first.\")\n",
        "\n",
        "run_meta = json.loads(run_meta_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "SHORTS_TARGET_SECONDS = int(run_meta.get(\"shorts_target_seconds\", 120))\n",
        "SHORTS_SOFT_MAX_SECONDS = int(run_meta.get(\"shorts_soft_max_seconds\", 175))\n",
        "\n",
        "# Defaults (overridden by pace profiles below; guidance only)\n",
        "DEFAULT_TARGET_WORDS_MIN = int(run_meta.get(\"lyrics_target_words_min\", 200))\n",
        "DEFAULT_TARGET_WORDS_MAX = int(run_meta.get(\"lyrics_target_words_max\", 320))\n",
        "DEFAULT_TARGET_LYRIC_LINES_MIN = int(run_meta.get(\"lyrics_target_lines_min\", 28))\n",
        "DEFAULT_TARGET_LYRIC_LINES_MAX = int(run_meta.get(\"lyrics_target_lines_max\", 44))\n",
        "\n",
        "FORMAT_ID = str(run_meta.get(\"format_id\", \"\") or \"\").strip()\n",
        "FORMAT_LABEL = str(run_meta.get(\"format_label\", \"\") or \"\").strip()\n",
        "\n",
        "# Prefer Block 3's explicit pace profile mapping if available\n",
        "PACE_PROFILE_ID = str(run_meta.get(\"pace_profile_id\", \"\") or \"\").strip()\n",
        "PACE_PROFILE_ID = PACE_PROFILE_ID or str(run_meta.get(\"lyrics_pace_profile\", {}).get(\"format_id\", \"\") or \"\").strip()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Pace-aware lyric budgets (GUIDANCE, not hard caps)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "PACE_PROFILES = {\n",
        "    \"rap_summary\": {\n",
        "        \"wpm_range\": (165, 210),\n",
        "        \"instrumental_padding_s\": 10,\n",
        "        \"label\": \"fast / rap-like delivery\",\n",
        "    },\n",
        "    \"science_pop_song\": {\n",
        "        \"wpm_range\": (135, 175),\n",
        "        \"instrumental_padding_s\": 15,\n",
        "        \"label\": \"medium-fast / upbeat\",\n",
        "    },\n",
        "    \"mindset_diary\": {\n",
        "        \"wpm_range\": (115, 155),\n",
        "        \"instrumental_padding_s\": 20,\n",
        "        \"label\": \"medium / measured explainer\",\n",
        "    },\n",
        "    \"epic_breakdown\": {\n",
        "        \"wpm_range\": (95, 135),\n",
        "        \"instrumental_padding_s\": 25,\n",
        "        \"label\": \"slow-medium / cinematic\",\n",
        "    },\n",
        "    \"techwave\": {\n",
        "        \"wpm_range\": (95, 140),\n",
        "        \"instrumental_padding_s\": 25,\n",
        "        \"label\": \"slow-medium / atmospheric\",\n",
        "    },\n",
        "    \"explain_like_im_5\": {\n",
        "        \"wpm_range\": (110, 150),\n",
        "        \"instrumental_padding_s\": 20,\n",
        "        \"label\": \"medium / plain-language\",\n",
        "    },\n",
        "    \"__default__\": {\n",
        "        \"wpm_range\": (120, 165),\n",
        "        \"instrumental_padding_s\": 18,\n",
        "        \"label\": \"default / balanced\",\n",
        "    },\n",
        "}\n",
        "\n",
        "def compute_pace_targets(\n",
        "    pace_key: str,\n",
        "    shorts_target_s: int,\n",
        "    shorts_soft_max_s: int,\n",
        "):\n",
        "    prof = PACE_PROFILES.get(pace_key) or PACE_PROFILES[\"__default__\"]\n",
        "    wpm_min, wpm_max = prof[\"wpm_range\"]\n",
        "    pad = int(prof[\"instrumental_padding_s\"])\n",
        "\n",
        "    target_total_s = int(shorts_target_s)\n",
        "    soft_max_total_s = int(shorts_soft_max_s)\n",
        "\n",
        "    safety_buffer_s = 8\n",
        "    target_vocals_s = max(60, target_total_s - pad - safety_buffer_s)\n",
        "    soft_max_vocals_s = max(75, soft_max_total_s - pad - safety_buffer_s)\n",
        "\n",
        "    words_min = int(math.floor((target_vocals_s / 60.0) * wpm_min))\n",
        "    words_max = int(math.ceil((soft_max_vocals_s / 60.0) * wpm_max))\n",
        "\n",
        "    words_min = max(140, min(words_min, 380))\n",
        "    words_max = max(words_min + 40, min(words_max, 520))\n",
        "\n",
        "    lines_min = max(20, int(words_min / 10))\n",
        "    lines_max = max(lines_min + 8, int(words_max / 7))\n",
        "\n",
        "    return {\n",
        "        \"pace_key\": pace_key,\n",
        "        \"profile_label\": prof[\"label\"],\n",
        "        \"wpm_range\": (wpm_min, wpm_max),\n",
        "        \"instrumental_padding_s\": pad,\n",
        "        \"target_vocals_s\": target_vocals_s,\n",
        "        \"soft_max_vocals_s\": soft_max_vocals_s,\n",
        "        \"target_words_min\": words_min,\n",
        "        \"target_words_max\": words_max,\n",
        "        \"target_lines_min\": lines_min,\n",
        "        \"target_lines_max\": lines_max,\n",
        "    }\n",
        "\n",
        "PACE_KEY = PACE_PROFILE_ID or FORMAT_ID or \"__default__\"\n",
        "pace_targets = compute_pace_targets(PACE_KEY, SHORTS_TARGET_SECONDS, SHORTS_SOFT_MAX_SECONDS)\n",
        "\n",
        "TARGET_WORDS_MIN = pace_targets[\"target_words_min\"]\n",
        "TARGET_WORDS_MAX = pace_targets[\"target_words_max\"]\n",
        "TARGET_LYRIC_LINES_MIN = pace_targets[\"target_lines_min\"]\n",
        "TARGET_LYRIC_LINES_MAX = pace_targets[\"target_lines_max\"]\n",
        "\n",
        "print(\"â±ï¸ Pace-aware lyric guidance loaded:\")\n",
        "print(f\"   - Format: {FORMAT_LABEL or FORMAT_ID or 'unknown'}\")\n",
        "print(f\"   - Pace key: {PACE_KEY}\")\n",
        "print(f\"   - Pace profile: {pace_targets['profile_label']}\")\n",
        "print(f\"   - WPM range: {pace_targets['wpm_range'][0]}â€“{pace_targets['wpm_range'][1]}\")\n",
        "print(f\"   - Reserved instrumental padding: ~{pace_targets['instrumental_padding_s']}s\")\n",
        "print(f\"   - Target vocal time: ~{pace_targets['target_vocals_s']}s \"\n",
        "      f\"(soft max vocals ~{pace_targets['soft_max_vocals_s']}s)\")\n",
        "print(f\"   - Lyric targets: {TARGET_WORDS_MIN}â€“{TARGET_WORDS_MAX} words, \"\n",
        "      f\"{TARGET_LYRIC_LINES_MIN}â€“{TARGET_LYRIC_LINES_MAX} lyric lines (excluding tags)\")\n",
        "\n",
        "# Persist computed targets into run_meta for debugging / tuning\n",
        "run_meta[\"lyrics_target_words_min\"] = TARGET_WORDS_MIN\n",
        "run_meta[\"lyrics_target_words_max\"] = TARGET_WORDS_MAX\n",
        "run_meta[\"lyrics_target_lines_min\"] = TARGET_LYRIC_LINES_MIN\n",
        "run_meta[\"lyrics_target_lines_max\"] = TARGET_LYRIC_LINES_MAX\n",
        "run_meta[\"lyrics_pace_profile\"] = {\n",
        "    \"pace_key\": PACE_KEY,\n",
        "    \"format_id\": FORMAT_ID,\n",
        "    \"format_label\": FORMAT_LABEL,\n",
        "    \"pace_profile_id\": PACE_PROFILE_ID,\n",
        "    \"profile_label\": pace_targets[\"profile_label\"],\n",
        "    \"wpm_range\": list(pace_targets[\"wpm_range\"]),\n",
        "    \"instrumental_padding_s\": pace_targets[\"instrumental_padding_s\"],\n",
        "    \"target_vocals_s\": pace_targets[\"target_vocals_s\"],\n",
        "    \"soft_max_vocals_s\": pace_targets[\"soft_max_vocals_s\"],\n",
        "}\n",
        "run_meta_path.write_text(json.dumps(run_meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Helpers\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "TAG_RE = re.compile(r\"^\\s*\\[[^\\]]+\\]\\s*$\")\n",
        "\n",
        "def count_words(text: str) -> int:\n",
        "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "    content_lines = [ln for ln in lines if not TAG_RE.match(ln)]\n",
        "    joined = \" \".join(content_lines)\n",
        "    tokens = re.findall(r\"[A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿0-9']+\", joined)\n",
        "    return len(tokens)\n",
        "\n",
        "def count_lyric_lines(text: str) -> int:\n",
        "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "    return sum(1 for ln in lines if not TAG_RE.match(ln))\n",
        "\n",
        "def clamp_bullets(bullets: List[str]) -> List[str]:\n",
        "    bullets = [b.strip(\"- \").strip() for b in bullets if b.strip()]\n",
        "    if len(bullets) < 6:\n",
        "        return bullets\n",
        "    if len(bullets) > 10:\n",
        "        return bullets[:10]\n",
        "    return bullets\n",
        "\n",
        "def normalize_bullets(bullets: List[str]) -> List[str]:\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for b in bullets:\n",
        "        b = re.sub(r\"\\s+\", \" \", b).strip()\n",
        "        b = b.rstrip(\";\")\n",
        "        bl = b.lower()\n",
        "        if bl and bl not in seen:\n",
        "            seen.add(bl)\n",
        "            out.append(b)\n",
        "    return out\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Step 1: teaching bullets (general-audience topic, deep explanation)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def generate_teaching_bullets(subject: str, language: str = LYRICS_LANGUAGE) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate 6â€“10 accurate teaching bullets for a broad audience topic,\n",
        "    but structured so the song can go deep (intuition â†’ mechanism â†’ example â†’ misconception).\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        f\"You are an expert teacher explaining '{subject}' to a general audience in {language}.\\n\"\n",
        "        \"Assume curiosity, not prior knowledge.\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- Output ONLY 6 to 10 bullet points.\\n\"\n",
        "        \"- Each bullet must be ONE sentence, accurate and concrete.\\n\"\n",
        "        \"- Cover, in order where possible:\\n\"\n",
        "        \"  (1) what it is (plain words),\\n\"\n",
        "        \"  (2) the core mechanism (how/why),\\n\"\n",
        "        \"  (3) a simple everyday example/intuition anchor,\\n\"\n",
        "        \"  (4) a trade-off/limit/edge case,\\n\"\n",
        "        \"  (5) a common misconception and the correction.\\n\"\n",
        "        \"- Define any key term briefly inside the bullet the first time it appears.\\n\"\n",
        "        \"- Avoid equations unless they are truly essential; prefer concepts.\\n\"\n",
        "        \"- Do NOT number them, just use '-' at the start of each line.\\n\"\n",
        "        \"- Avoid claims that require citations, statistics, or named studies.\\n\"\n",
        "    )\n",
        "\n",
        "    resp = openai_client.chat.completions.create(\n",
        "        model=GPT_LYRICS_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.25,\n",
        "        max_tokens=700,\n",
        "    )\n",
        "\n",
        "    text = resp.choices[0].message.content.strip()\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "\n",
        "    bullets = []\n",
        "    for line in lines:\n",
        "        if line.startswith(\"-\"):\n",
        "            line = line[1:].strip()\n",
        "        bullets.append(line)\n",
        "\n",
        "    bullets = normalize_bullets(clamp_bullets(bullets))\n",
        "    if len(bullets) < 6:\n",
        "        raise ValueError(f\"Got too few bullets from GPT: {bullets}\")\n",
        "\n",
        "    return bullets\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Step 2: bullets â†’ dense, rhyming explainer lyrics (pace-aware)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def bullets_to_lyrics(\n",
        "    bullets: List[str],\n",
        "    subject: str,\n",
        "    persona: str,\n",
        "    tone: str,\n",
        "    language: str = LYRICS_LANGUAGE,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    General-topic, deep explainer lyrics:\n",
        "    - intuition-first but still dense and accurate\n",
        "    - defines terms briefly on first use\n",
        "    - minimal repetition\n",
        "    - guided by pace-aware word budgets\n",
        "    \"\"\"\n",
        "    bullets_text = \"\\n\".join(f\"- {b}\" for b in bullets)\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are writing educational lyrics in {language} for YouTube Shorts.\\n\"\n",
        "        f\"Audience: general audience (teens+), curious, not experts.\\n\"\n",
        "        f\"Subject: '{subject}'.\\n\\n\"\n",
        "        \"You MUST cover these teaching points (do not add new claims beyond them):\\n\"\n",
        "        f\"{bullets_text}\\n\\n\"\n",
        "        \"Lyric style requirements:\\n\"\n",
        "        \"- Make it Suno-friendly: clean, singable, rhythmic.\\n\"\n",
        "        \"- Start with intuition (what it is + why it matters), then go mechanism.\\n\"\n",
        "        \"- When a technical term appears first time, define it in a short, plain line.\\n\"\n",
        "        \"- Use INTERNAL RHYMES and END RHYMES when natural, but never sacrifice clarity or accuracy.\\n\"\n",
        "        \"- Keep explanations crisp: prefer short lines; no long multi-clause lines.\\n\"\n",
        "        \"- MINIMIZE repetition: if you include a hook, repeat it at most once.\\n\"\n",
        "        \"- Avoid long intros/outros; begin explaining quickly.\\n\"\n",
        "        \"- Use section tags on their own lines, but keep it simple:\\n\"\n",
        "        \"  [Intro]\\n\"\n",
        "        \"  [Explain]\\n\"\n",
        "        \"  [Explain 2]\\n\"\n",
        "        \"  [Hook] (optional, short)\\n\"\n",
        "        \"  [Final]\\n\\n\"\n",
        "        \"Pace + length guidance (do NOT mention numbers in the lyrics):\\n\"\n",
        "        f\"- Target total lyric words: {TARGET_WORDS_MIN}â€“{TARGET_WORDS_MAX} (excluding tag lines).\\n\"\n",
        "        f\"- Target lyric lines: {TARGET_LYRIC_LINES_MIN}â€“{TARGET_LYRIC_LINES_MAX} (excluding tag lines).\\n\\n\"\n",
        "        \"Musical context:\\n\"\n",
        "        f\"- Style / vibe: {persona}.\\n\"\n",
        "        f\"- Tone: {tone}.\\n\\n\"\n",
        "        \"Output rules:\\n\"\n",
        "        \"- Return ONLY the lyrics with section tags.\\n\"\n",
        "        \"- No commentary, no bullet points, no emojis.\\n\"\n",
        "    )\n",
        "\n",
        "    resp = openai_client.chat.completions.create(\n",
        "        model=GPT_LYRICS_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.85,\n",
        "        max_tokens=1700,\n",
        "    )\n",
        "\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Step 3: gentle length correction pass (guidance, not brute force)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def maybe_adjust_lyrics_to_targets(lyrics: str, bullets: List[str], language: str = LYRICS_LANGUAGE) -> str:\n",
        "    w = count_words(lyrics)\n",
        "    n_lines = count_lyric_lines(lyrics)\n",
        "\n",
        "    too_long = (w > TARGET_WORDS_MAX) or (n_lines > TARGET_LYRIC_LINES_MAX)\n",
        "    too_short = (w < TARGET_WORDS_MIN) or (n_lines < TARGET_LYRIC_LINES_MIN)\n",
        "\n",
        "    if not too_long and not too_short:\n",
        "        return lyrics\n",
        "\n",
        "    direction = \"compress\" if too_long else \"expand slightly\"\n",
        "    bullets_text = \"\\n\".join(f\"- {b}\" for b in bullets)\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are editing educational lyrics in {language}.\\n\"\n",
        "        f\"Goal: {direction} while keeping clarity for a general audience and keeping musical flow.\\n\\n\"\n",
        "        \"Constraints:\\n\"\n",
        "        \"- Keep section tags on their own lines.\\n\"\n",
        "        \"- Keep it Suno-friendly and rhythmic.\\n\"\n",
        "        \"- Keep light rhyme where natural.\\n\"\n",
        "        \"- Avoid repetition (no chorus loops).\\n\"\n",
        "        \"- Do NOT introduce new factual claims.\\n\"\n",
        "        \"- You MUST still cover these teaching points:\\n\"\n",
        "        f\"{bullets_text}\\n\\n\"\n",
        "        \"Length guidance (do not mention numbers in the lyrics):\\n\"\n",
        "        f\"- Target words: {TARGET_WORDS_MIN}â€“{TARGET_WORDS_MAX} (excluding tag lines)\\n\"\n",
        "        f\"- Target lines: {TARGET_LYRIC_LINES_MIN}â€“{TARGET_LYRIC_LINES_MAX} (excluding tag lines)\\n\\n\"\n",
        "        \"Here are the lyrics to revise:\\n\"\n",
        "        f\"{lyrics}\\n\\n\"\n",
        "        \"Return ONLY the revised lyrics.\"\n",
        "    )\n",
        "\n",
        "    resp = openai_client.chat.completions.create(\n",
        "        model=GPT_LYRICS_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.6,\n",
        "        max_tokens=1700,\n",
        "    )\n",
        "\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Run the lyrics pipeline for the current run\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print(\"ğŸ§  Generating teaching bullets (intuition-first, deep)...\")\n",
        "bullets = generate_teaching_bullets(subject, language=LYRICS_LANGUAGE)\n",
        "\n",
        "bullets_path = RUN_DIR / \"logs\" / \"teaching_bullets.txt\"\n",
        "bullets_path.write_text(\"\\n\".join(f\"- {b}\" for b in bullets), encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Bullets generated:\")\n",
        "for b in bullets:\n",
        "    print(\"  -\", b)\n",
        "print(f\"   Saved to: {bullets_path}\")\n",
        "\n",
        "print(\"\\nğŸ¼ Generating pace-aware explainer lyrics (general topic, deep explanation)...\")\n",
        "lyrics_raw = bullets_to_lyrics(bullets, subject, persona, tone, language=LYRICS_LANGUAGE)\n",
        "lyrics = maybe_adjust_lyrics_to_targets(lyrics_raw, bullets=bullets, language=LYRICS_LANGUAGE)\n",
        "\n",
        "final_words = count_words(lyrics)\n",
        "final_lines = count_lyric_lines(lyrics)\n",
        "print(f\"\\nğŸ“ Lyric stats (excluding tags): {final_words} words, {final_lines} lyric lines\")\n",
        "print(f\"ğŸšï¸ Pace profile used: {pace_targets['profile_label']} (pace_key={PACE_KEY})\")\n",
        "\n",
        "lyrics_path = RUN_DIR / \"lyrics.txt\"\n",
        "lyrics_path.write_text(lyrics, encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Lyrics generated and saved:\")\n",
        "print(f\"   {lyrics_path}\")\n",
        "print(\"\\nâ”€â”€â”€â”€ Sample of lyrics (first ~800 characters) â”€â”€â”€â”€\")\n",
        "print(lyrics[:800])\n",
        "print(\"\\n(Full lyrics are saved to lyrics.txt in the run folder.)\")\n"
      ],
      "metadata": {
        "id": "nBO5vAWgf0v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 5. Suno Custom Mode: generate song from lyrics and download\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Use your known-working base + model\n",
        "SUNO_BASE = config.suno_base_url.rstrip(\"/\")  # from Block 1 config\n",
        "SUNO_MODEL = \"V5\"                             # adjust if you want another Suno model\n",
        "\n",
        "def suno_headers():\n",
        "    return {\n",
        "        \"Authorization\": f\"Bearer {config.suno_api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "def suno_generate_custom(lyrics: str, subject: str, persona: str, model: str = SUNO_MODEL) -> str:\n",
        "    \"\"\"\n",
        "    Custom mode: we provide full lyrics, and Suno sings them in the given style.\n",
        "    Returns a taskId we can poll.\n",
        "    \"\"\"\n",
        "    # Keep title short & YouTube-friendly â€“ can be improved later\n",
        "    title = f\"{subject.title()} explained in a song\"\n",
        "    # Use persona as style hint\n",
        "    style = persona\n",
        "\n",
        "    payload = {\n",
        "        \"customMode\": True,\n",
        "        \"instrumental\": False,   # we want vocals singing our lyrics\n",
        "        \"model\": model,\n",
        "        \"title\": title[:100],    # avoid overly long titles\n",
        "        \"style\": style[:200],\n",
        "        \"prompt\": lyrics,        # our generated lyrics become the sung text\n",
        "\n",
        "        # Your Suno provider requires a callBackUrl field in the payload,\n",
        "        # even if we don't actually use webhooks and instead poll manually.\n",
        "        \"callBackUrl\": \"https://example.com/callback\"\n",
        "    }\n",
        "\n",
        "    print(\"ğŸµ Sending Custom Mode generate request to Suno...\")\n",
        "    r = requests.post(f\"{SUNO_BASE}/generate\", headers=suno_headers(), json=payload, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    j = r.json()\n",
        "\n",
        "    # Many Suno-compatible APIs return { code: 200, data: { taskId: ... } }\n",
        "    if j.get(\"code\") != 200:\n",
        "        raise RuntimeError(f\"Suno generate error: {j}\")\n",
        "\n",
        "    task_id = j[\"data\"][\"taskId\"]\n",
        "    print(\"âœ… Suno task created:\", task_id)\n",
        "    return task_id\n",
        "\n",
        "def suno_record_info(task_id: str):\n",
        "    \"\"\"\n",
        "    Query Suno for the status and metadata of a generation task.\n",
        "    \"\"\"\n",
        "    r = requests.get(\n",
        "        f\"{SUNO_BASE}/generate/record-info\",\n",
        "        headers=suno_headers(),\n",
        "        params={\"taskId\": task_id},\n",
        "        timeout=60,\n",
        "    )\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def wait_for_suno_song(task_id: str, timeout_s: int = 360, poll_every: int = 5):\n",
        "    \"\"\"\n",
        "    Poll the Suno task until it finishes or times out.\n",
        "    Returns the final record-info JSON.\n",
        "    \"\"\"\n",
        "    print(\"â³ Waiting for Suno to finish generation...\")\n",
        "    t0 = time.time()\n",
        "    status = \"PENDING\"\n",
        "    last = None\n",
        "\n",
        "    while time.time() - t0 < timeout_s:\n",
        "        last = suno_record_info(task_id)\n",
        "        data = last.get(\"data\", {})\n",
        "        status = data.get(\"status\")\n",
        "        print(f\"  - Status: {status}\")\n",
        "\n",
        "        if status in (\"SUCCESS\", \"FIRST_SUCCESS\"):\n",
        "            print(\"âœ… Suno generation completed.\")\n",
        "            return last\n",
        "\n",
        "        time.sleep(poll_every)\n",
        "\n",
        "    raise TimeoutError(f\"Suno task timeout. Last status={status}, payload={last}\")\n",
        "\n",
        "\n",
        "def download_file(url: str, out_path: pathlib.Path):\n",
        "    \"\"\"\n",
        "    Download a file from URL to out_path using streaming.\n",
        "    \"\"\"\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with requests.get(url, stream=True, timeout=120) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "\n",
        "def get_track_with_audio(task_id: str, initial_rec: dict, max_retries: int = 5, retry_delay: int = 5):\n",
        "    \"\"\"\n",
        "    Some Suno tenants mark status SUCCESS before audioUrl is actually populated.\n",
        "    This helper:\n",
        "      - Looks for the first track with a non-empty audioUrl\n",
        "      - If not found, re-polls record-info a few times with a delay\n",
        "    \"\"\"\n",
        "    rec = initial_rec\n",
        "    for attempt in range(max_retries + 1):\n",
        "        data = rec.get(\"data\", {})\n",
        "        response = data.get(\"response\", {})\n",
        "        suno_data = response.get(\"sunoData\", []) or []\n",
        "\n",
        "        tracks_with_url = [t for t in suno_data if t.get(\"audioUrl\")]\n",
        "        if tracks_with_url:\n",
        "            if attempt > 0:\n",
        "                print(f\"âœ… Audio URL available after {attempt} extra check(s).\")\n",
        "            return tracks_with_url[0]\n",
        "\n",
        "        if attempt < max_retries:\n",
        "            print(f\"âš ï¸ audioUrl not ready yet (attempt {attempt+1}/{max_retries}), retrying...\")\n",
        "            time.sleep(retry_delay)\n",
        "            rec = suno_record_info(task_id)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    raise RuntimeError(f\"No track with a valid audioUrl found after retries. Last payload: {rec}\")\n",
        "\n",
        "\n",
        "# â”€â”€ Run Suno generation for this run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Load lyrics from the file we saved in Block 4\n",
        "lyrics_path = RUN_DIR / \"lyrics.txt\"\n",
        "if not lyrics_path.exists():\n",
        "    raise FileNotFoundError(f\"lyrics.txt not found at {lyrics_path}. Run Block 4 first.\")\n",
        "\n",
        "lyrics_text = lyrics_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "# 1) Create the generation task\n",
        "task_id = suno_generate_custom(lyrics_text, subject=subject, persona=persona, model=SUNO_MODEL)\n",
        "\n",
        "# 2) Poll until the song is ready\n",
        "rec = wait_for_suno_song(task_id)\n",
        "\n",
        "# 3) Extract track info with robust audioUrl handling\n",
        "track = get_track_with_audio(task_id, rec)\n",
        "AUDIO_URL = track[\"audioUrl\"]\n",
        "AUDIO_ID  = track[\"id\"]\n",
        "TITLE     = track.get(\"title\", \"Untitled\")\n",
        "DURATION  = track.get(\"duration\", None)\n",
        "\n",
        "# 4) Download audio into this run's audio folder\n",
        "audio_path = RUN_DIR / \"audio\" / \"song.mp3\"\n",
        "download_file(AUDIO_URL, audio_path)\n",
        "\n",
        "# 5) Save metadata for later steps (subtitles, video, upload)\n",
        "song_meta = {\n",
        "    \"task_id\": task_id,\n",
        "    \"audio_id\": AUDIO_ID,\n",
        "    \"title\": TITLE,\n",
        "    \"duration\": DURATION,\n",
        "    \"audio_url\": AUDIO_URL,\n",
        "    \"subject\": subject,\n",
        "    \"persona\": persona,\n",
        "    \"tone\": tone,\n",
        "}\n",
        "\n",
        "song_meta_path = RUN_DIR / \"logs\" / \"suno_song_meta.json\"\n",
        "song_meta_path.write_text(json.dumps(song_meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "print(\"ğŸ§ Suno song ready!\")\n",
        "print(\"  Title   :\", TITLE)\n",
        "print(\"  AudioID :\", AUDIO_ID)\n",
        "print(\"  Duration:\", DURATION)\n",
        "print(\"  Audio   :\", audio_path)\n",
        "print(f\"  Meta    : {song_meta_path}\")\n"
      ],
      "metadata": {
        "id": "0pdZogMYhcEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 6. Timestamped lyrics (Suno) â†’ word groups for subtitles\n",
        "#    (Retry + validation; max wait ~5 minutes)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "\n",
        "# We reuse:\n",
        "# - SUNO_BASE\n",
        "# - suno_headers()\n",
        "# - RUN_DIR\n",
        "# - requests\n",
        "\n",
        "# Subtitle grouping parameters:\n",
        "# - More words per caption for readability\n",
        "# - Also use a rough max duration per caption\n",
        "CAP_WORDS_MIN = 5\n",
        "CAP_WORDS_MAX = 10\n",
        "CAP_MAX_GAP   = 0.25   # max silence gap (seconds) allowed inside one caption\n",
        "CAP_TARGET_MAX_DUR = 3.0  # seconds; aim to keep each caption under this\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Retry settings\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "LYRICS_MAX_WAIT_S = 300          # total cap: 5 minutes\n",
        "LYRICS_RETRY_DELAY_S = 8         # base delay between tries\n",
        "LYRICS_RETRY_JITTER_S = 4        # add 0â€“4s randomness\n",
        "LYRICS_REQUEST_TIMEOUT_S = 60    # timeout per request\n",
        "LYRICS_MIN_WORDS_OK = 40         # sanity check: must look like real alignment\n",
        "\n",
        "\n",
        "def suno_aligned_words_once(task_id: str, audio_id: str):\n",
        "    \"\"\"\n",
        "    Single attempt: call Suno timestamped lyrics endpoint.\n",
        "    Returns (alignedWords_or_None, raw_json).\n",
        "    \"\"\"\n",
        "    url = f\"{SUNO_BASE}/generate/get-timestamped-lyrics\"\n",
        "    payload = {\"taskId\": task_id, \"audioId\": audio_id}\n",
        "\n",
        "    r = requests.post(url, headers=suno_headers(), json=payload, timeout=LYRICS_REQUEST_TIMEOUT_S)\n",
        "    r.raise_for_status()\n",
        "    j = r.json()\n",
        "\n",
        "    aligned = j.get(\"data\", {}).get(\"alignedWords\")\n",
        "    if j.get(\"code\") == 200 and aligned:\n",
        "        return aligned, j\n",
        "\n",
        "    return None, j\n",
        "\n",
        "\n",
        "def aligned_words_to_WORDS(aligned):\n",
        "    \"\"\"\n",
        "    Convert Suno alignedWords to our WORDS format.\n",
        "    Filters out empty words and coerces types safely.\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    for a in aligned or []:\n",
        "        w = str(a.get(\"word\", \"\")).strip()\n",
        "        if not w:\n",
        "            continue\n",
        "        if not a.get(\"success\", True):\n",
        "            continue\n",
        "        try:\n",
        "            start = float(a.get(\"startS\"))\n",
        "            end   = float(a.get(\"endS\"))\n",
        "        except Exception:\n",
        "            continue\n",
        "        if end <= start:\n",
        "            continue\n",
        "        words.append({\"w\": w, \"start\": start, \"end\": end})\n",
        "    return words\n",
        "\n",
        "\n",
        "def words_look_valid(words) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristics to confirm we actually got real aligned lyrics.\n",
        "    Prevents 'empty' or partial junk from passing.\n",
        "    \"\"\"\n",
        "    if not words or len(words) < LYRICS_MIN_WORDS_OK:\n",
        "        return False\n",
        "\n",
        "    # Basic monotonic time sanity check\n",
        "    last_end = -1.0\n",
        "    bad = 0\n",
        "    for x in words[:200]:  # sample first 200 for speed\n",
        "        if x[\"start\"] < 0 or x[\"end\"] < 0:\n",
        "            bad += 1\n",
        "        if x[\"start\"] < last_end - 0.05:\n",
        "            bad += 1\n",
        "        last_end = max(last_end, x[\"end\"])\n",
        "\n",
        "    if bad > 5:\n",
        "        return False\n",
        "\n",
        "    # Must contain some alphabetic content overall (avoid weird non-lyrics payload)\n",
        "    sample_text = \" \".join(w[\"w\"] for w in words[:60])\n",
        "    if not any(ch.isalpha() for ch in sample_text):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def fetch_suno_words_with_retry(task_id: str, audio_id: str, max_wait_s: int = LYRICS_MAX_WAIT_S):\n",
        "    \"\"\"\n",
        "    Keep trying until we get valid WORDS or we hit max_wait_s.\n",
        "    Returns (WORDS_or_None, debug_info_dict).\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    attempt = 0\n",
        "    last_response_preview = None\n",
        "\n",
        "    print(\"ğŸ“ Fetching Suno timestamped lyrics (retry up to ~5 minutes)...\")\n",
        "\n",
        "    while True:\n",
        "        attempt += 1\n",
        "        elapsed = int(time.time() - t0)\n",
        "\n",
        "        if elapsed >= max_wait_s:\n",
        "            print(f\"â­ï¸ Gave up after {elapsed}s (max {max_wait_s}s). Proceeding WITHOUT subtitles.\")\n",
        "            return None, {\n",
        "                \"attempts\": attempt - 1,\n",
        "                \"elapsed_s\": elapsed,\n",
        "                \"status\": \"timeout_no_valid_alignment\",\n",
        "                \"last_response_preview\": last_response_preview,\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            aligned, raw = suno_aligned_words_once(task_id, audio_id)\n",
        "\n",
        "            last_response_preview = {\n",
        "                \"code\": raw.get(\"code\"),\n",
        "                \"msg\": raw.get(\"msg\") or raw.get(\"message\"),\n",
        "                \"has_alignedWords\": bool(raw.get(\"data\", {}).get(\"alignedWords\")),\n",
        "            }\n",
        "\n",
        "            if aligned:\n",
        "                words = aligned_words_to_WORDS(aligned)\n",
        "                if words_look_valid(words):\n",
        "                    print(f\"âœ… Valid aligned lyrics received on attempt {attempt} (elapsed ~{elapsed}s).\")\n",
        "                    return words, {\n",
        "                        \"attempts\": attempt,\n",
        "                        \"elapsed_s\": elapsed,\n",
        "                        \"status\": \"success\",\n",
        "                        \"last_response_preview\": last_response_preview,\n",
        "                    }\n",
        "                else:\n",
        "                    print(f\"âš ï¸ Attempt {attempt}: alignedWords returned but failed validation (len={len(words)}).\")\n",
        "            else:\n",
        "                print(f\"âš ï¸ Attempt {attempt}: no alignedWords yet (elapsed ~{elapsed}s).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Attempt {attempt}: request failed: {e}\")\n",
        "\n",
        "        # Wait before retry (jittered), without oversleeping past max_wait_s\n",
        "        sleep_s = LYRICS_RETRY_DELAY_S + random.randint(0, LYRICS_RETRY_JITTER_S)\n",
        "        remaining = max_wait_s - (time.time() - t0)\n",
        "        sleep_s = min(sleep_s, max(0, remaining))\n",
        "        if sleep_s > 0:\n",
        "            time.sleep(sleep_s)\n",
        "\n",
        "\n",
        "def group_words(words, min_k=5, max_k=10, max_gap=0.25, target_max_dur=3.0):\n",
        "    \"\"\"\n",
        "    Group word-level timestamps into longer subtitle chunks.\n",
        "\n",
        "    Goals:\n",
        "    - 5â€“10 words per caption\n",
        "    - Don't cross large gaps of silence (> max_gap)\n",
        "    - Try to keep each caption under target_max_dur seconds\n",
        "    \"\"\"\n",
        "    groups = []\n",
        "    i = 0\n",
        "    n = len(words)\n",
        "\n",
        "    while i < n:\n",
        "        start_time = words[i][\"start\"]\n",
        "        j = i + 1\n",
        "\n",
        "        while j < n:\n",
        "            num = j - i\n",
        "            current_end = words[j - 1][\"end\"]\n",
        "            dur = current_end - start_time\n",
        "            gap_to_next = words[j][\"start\"] - words[j - 1][\"end\"] if j < n else 0.0\n",
        "\n",
        "            if num >= max_k:\n",
        "                break\n",
        "            if dur >= target_max_dur:\n",
        "                break\n",
        "            if gap_to_next > max_gap:\n",
        "                break\n",
        "\n",
        "            j += 1\n",
        "\n",
        "        while (j < n) and ((j - i) < min_k):\n",
        "            gap_to_next = words[j][\"start\"] - words[j - 1][\"end\"]\n",
        "            if gap_to_next > max_gap:\n",
        "                break\n",
        "            j += 1\n",
        "\n",
        "        chunk = words[i:j]\n",
        "        txt = \" \".join(w[\"w\"] for w in chunk)\n",
        "        start = chunk[0][\"start\"]\n",
        "        end = chunk[-1][\"end\"]\n",
        "        groups.append({\"text\": txt, \"start\": start, \"end\": end})\n",
        "\n",
        "        i = j\n",
        "\n",
        "    return groups\n",
        "\n",
        "\n",
        "# â”€â”€ Load Suno song metadata (from Block 5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "song_meta_path = RUN_DIR / \"logs\" / \"suno_song_meta.json\"\n",
        "if not song_meta_path.exists():\n",
        "    raise FileNotFoundError(f\"suno_song_meta.json not found at {song_meta_path}. Run Block 5 first.\")\n",
        "\n",
        "song_meta = json.loads(song_meta_path.read_text(encoding=\"utf-8\"))\n",
        "task_id = song_meta[\"task_id\"]\n",
        "audio_id = song_meta[\"audio_id\"]\n",
        "\n",
        "# â”€â”€ Fetch aligned words from Suno (with retry) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "WORDS, lyrics_debug = fetch_suno_words_with_retry(task_id, audio_id, max_wait_s=LYRICS_MAX_WAIT_S)\n",
        "\n",
        "# Save debug info always\n",
        "debug_path = RUN_DIR / \"logs\" / \"aligned_words_fetch_debug.json\"\n",
        "debug_path.write_text(json.dumps(lyrics_debug, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "print(f\"ğŸ§¾ Saved fetch debug info to: {debug_path}\")\n",
        "\n",
        "# Save raw aligned words (if any) for debugging\n",
        "aligned_words_path = RUN_DIR / \"logs\" / \"aligned_words.json\"\n",
        "aligned_words_path.write_text(json.dumps(WORDS or [], indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "print(f\"ğŸ’¾ Saved raw aligned words to: {aligned_words_path}\")\n",
        "\n",
        "# â”€â”€ Group into subtitle bursts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "GROUPS = []\n",
        "if WORDS:\n",
        "    GROUPS = group_words(\n",
        "        WORDS,\n",
        "        min_k=CAP_WORDS_MIN,\n",
        "        max_k=CAP_WORDS_MAX,\n",
        "        max_gap=CAP_MAX_GAP,\n",
        "        target_max_dur=CAP_TARGET_MAX_DUR,\n",
        "    )\n",
        "    print(f\"ğŸŸ¨ Groups created: {len(GROUPS)} (first few):\")\n",
        "    for g in GROUPS[:3]:\n",
        "        print(f\"  - [{g['start']:.2f}â€“{g['end']:.2f}] {g['text']}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No valid WORDS available, GROUPS will be empty and subtitles will be skipped.\")\n",
        "\n",
        "groups_path = RUN_DIR / \"logs\" / \"groups.json\"\n",
        "groups_path.write_text(json.dumps(GROUPS, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "print(f\"ğŸ’¾ Saved grouped subtitles to: {groups_path}\")\n"
      ],
      "metadata": {
        "id": "bkJ5x2Q8ksRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 7. Write ASS subtitles with glow + shadow (no dark bar)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "from pathlib import Path\n",
        "import math\n",
        "import json\n",
        "\n",
        "ASS_PATH = RUN_DIR / \"subs.ass\"\n",
        "\n",
        "# Ensure video constants exist (so Block 7 can run even if earlier cells didnâ€™t define them)\n",
        "if \"VIDEO_W\" not in globals():\n",
        "    VIDEO_W = 1080\n",
        "if \"VIDEO_H\" not in globals():\n",
        "    VIDEO_H = 1920\n",
        "if \"FPS\" not in globals():\n",
        "    FPS = 30\n",
        "\n",
        "# Ensure GROUPS exists (created in the earlier lyricâ†’timing grouping step)\n",
        "if \"GROUPS\" not in globals():\n",
        "    raise NameError(\"GROUPS is not defined. Run the block that creates subtitle GROUPS before Block 7.\")\n",
        "\n",
        "def ass_time(t: float) -> str:\n",
        "    # ASS expects H:MM:SS.CS (centiseconds)\n",
        "    if t < 0:\n",
        "        t = 0\n",
        "    cs = int(round((t - math.floor(t)) * 100))\n",
        "    sec = int(t) % 60\n",
        "    m = (int(t) // 60) % 60\n",
        "    h = int(t) // 3600\n",
        "    if cs == 100:\n",
        "        cs = 0\n",
        "        sec += 1\n",
        "    return f\"{h}:{m:02d}:{sec:02d}.{cs:02d}\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Single-layer subtitle style (white text with glow + shadow)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ASS_HEADER = f\"\"\"[Script Info]\n",
        "ScriptType: v4.00+\n",
        "PlayResX: {VIDEO_W}\n",
        "PlayResY: {VIDEO_H}\n",
        "ScaledBorderAndShadow: yes\n",
        "\n",
        "[V4+ Styles]\n",
        "Format: Name,Fontname,Fontsize,PrimaryColour,SecondaryColour,OutlineColour,BackColour,Bold,Italic,Underline,StrikeOut,ScaleX,ScaleY,Spacing,Angle,BorderStyle,Outline,Shadow,Alignment,MarginL,MarginR,MarginV,Encoding\n",
        "Style: FG,DejaVu Sans,56,&H00FFFFFF,&H00FFFFFF,&H00000000,&H32000000,-1,0,0,0,100,100,0,0,1,4,6,5,5,30,30,20,1\n",
        "\n",
        "[Events]\n",
        "Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n",
        "\"\"\"\n",
        "\n",
        "def write_ass(groups):\n",
        "    lines = [ASS_HEADER]\n",
        "\n",
        "    for g in groups:\n",
        "        text = str(g[\"text\"]).replace(\"\\n\", \" \").strip()\n",
        "        start = ass_time(float(g[\"start\"]))\n",
        "        end = ass_time(float(g[\"end\"]))\n",
        "\n",
        "        # Foreground glowing text only (no BG bar)\n",
        "        fg_line = (\n",
        "            f\"Dialogue: 0,{start},{end},FG,,0,0,40,,\"\n",
        "            f\"{{\\\\an5\\\\bord4\\\\blur2\\\\shad2}}{text}\"\n",
        "        )\n",
        "        lines.append(fg_line)\n",
        "\n",
        "    ASS_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "\n",
        "# Clamp cue timings and validate\n",
        "for g in GROUPS:\n",
        "    g[\"start\"] = max(0.0, float(g[\"start\"]))\n",
        "    g[\"end\"] = float(g[\"end\"])\n",
        "\n",
        "GROUPS = [g for g in GROUPS if g[\"end\"] > g[\"start\"] + 0.05]\n",
        "\n",
        "write_ass(GROUPS)\n",
        "print(\"âœ¨ ASS subtitles (glow only, no bar) written to:\", ASS_PATH)\n",
        "\n",
        "# preview first lines\n",
        "print(\"\\n\".join(ASS_PATH.read_text(encoding=\"utf-8\").splitlines()[:20]))\n"
      ],
      "metadata": {
        "id": "dKWoycyilETX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 8. Generate background image (OpenAI) + render dynamic MP4\n",
        "#    - Gentle zoom in / out loop\n",
        "#    - ASS subtitles overlaid via `subtitles` filter\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import base64\n",
        "import subprocess\n",
        "import json\n",
        "import math\n",
        "\n",
        "# We reuse:\n",
        "# - openai_client\n",
        "# - subject, persona, tone, LYRICS_LANGUAGE\n",
        "# - RUN_DIR\n",
        "# - audio_path (from Block 5)\n",
        "# - ASS_PATH, VIDEO_W, VIDEO_H, FPS (from Block 7)\n",
        "\n",
        "BACKGROUND_IMG_PATH = RUN_DIR / \"images\" / \"background.png\"\n",
        "FINAL_MP4 = RUN_DIR / \"video\" / \"shorts_final.mp4\"\n",
        "\n",
        "# Sanity checks\n",
        "if not audio_path.exists():\n",
        "    raise FileNotFoundError(f\"Audio file not found at {audio_path}. Run Block 5 first.\")\n",
        "if not ASS_PATH.exists():\n",
        "    raise FileNotFoundError(f\"ASS subtitles not found at {ASS_PATH}. Run Block 7 first.\")\n",
        "\n",
        "# Try to get audio duration from Suno meta (not strictly needed for zoom, but useful to log)\n",
        "song_meta_path = RUN_DIR / \"logs\" / \"suno_song_meta.json\"\n",
        "if song_meta_path.exists():\n",
        "    song_meta = json.loads(song_meta_path.read_text(encoding=\"utf-8\"))\n",
        "    audio_duration = song_meta.get(\"duration\", None)\n",
        "else:\n",
        "    audio_duration = None\n",
        "\n",
        "if not audio_duration or audio_duration <= 0:\n",
        "    audio_duration = 90.0  # soft default for logging / future use\n",
        "\n",
        "def build_background_prompt(subject: str, persona: str, tone: str, language: str = LYRICS_LANGUAGE) -> str:\n",
        "    \"\"\"\n",
        "    Build a descriptive prompt for a portrait background image.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        f\"Vertical illustration about {subject} in the style of {persona}, \"\n",
        "        f\"with a {tone} mood. High quality, cinematic, soft lighting, \"\n",
        "        \"no text, no logos, no UI, no watermarks. Composition leaves free space in the center \"\n",
        "        \"so subtitles are readable.\"\n",
        "    )\n",
        "\n",
        "def generate_background_image(prompt: str, out_path: pathlib.Path):\n",
        "    \"\"\"\n",
        "    Use OpenAI image model to generate a portrait background image.\n",
        "    Valid sizes: 1024x1024, 1024x1536, 1536x1024, auto.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ–¼ Generating background image with OpenAI...\")\n",
        "\n",
        "    response = openai_client.images.generate(\n",
        "        model=\"gpt-image-1\",\n",
        "        prompt=prompt,\n",
        "        size=\"1024x1536\",   # portrait ratio\n",
        "        n=1\n",
        "    )\n",
        "\n",
        "    b64 = response.data[0].b64_json\n",
        "    img_bytes = base64.b64decode(b64)\n",
        "\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_path.write_bytes(img_bytes)\n",
        "    print(f\"âœ… Background image saved to: {out_path}\")\n",
        "\n",
        "# â”€â”€ 1) Generate background image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "bg_prompt = build_background_prompt(subject, persona, tone)\n",
        "print(\"ğŸ§¾ Background prompt:\")\n",
        "print(\" \", bg_prompt)\n",
        "\n",
        "generate_background_image(bg_prompt, BACKGROUND_IMG_PATH)\n",
        "\n",
        "# â”€â”€ 2) Render final vertical MP4 with gentle zoom in/out + subtitles â”€â”€â”€â”€â”€\n",
        "\n",
        "# We:\n",
        "# - scale the source a bit larger than the final frame\n",
        "# - apply zoompan with an oscillating zoom:\n",
        "#     zoom(t) â‰ˆ 1.0 â†’ 1.07 â†’ 1.0 â†’ ...\n",
        "#   using frame number `on` (zoompan's output frame index)\n",
        "# - keep the image centered while zooming\n",
        "# - overlay ASS subtitles with the `subtitles` filter\n",
        "\n",
        "# Make the base image a bit larger so we can zoom >1.0 without quality loss\n",
        "BASE_W = int(VIDEO_W * 1.2)\n",
        "BASE_H = int(VIDEO_H * 1.2)\n",
        "\n",
        "# Zoom expression:\n",
        "# Centered at ~1.035 with amplitude 0.035 â†’ range ~1.0 to ~1.07 (5â€“7% zoom)\n",
        "zoom_expr = \"1.035+0.035*sin(on/120)\"\n",
        "\n",
        "# Center the zoom around the middle of the frame\n",
        "x_expr = \"iw/2 - iw/zoom/2\"\n",
        "y_expr = \"ih/2 - ih/zoom/2\"\n",
        "\n",
        "# Build the filter_complex string\n",
        "filter_complex = (\n",
        "    f\"[0:v]\"\n",
        "    f\"scale={BASE_W}:{BASE_H},\"\n",
        "    f\"zoompan=\"\n",
        "        f\"z='{zoom_expr}':\"\n",
        "        f\"x='{x_expr}':\"\n",
        "        f\"y='{y_expr}':\"\n",
        "        f\"d=1:\"\n",
        "        f\"s={VIDEO_W}x{VIDEO_H}:\"\n",
        "        f\"fps={FPS},\"\n",
        "    f\"subtitles='{ASS_PATH.as_posix()}'\"\n",
        "    f\"[v]\"\n",
        ")\n",
        "\n",
        "ffmpeg_cmd = f\"\"\"\n",
        "ffmpeg -y -loop 1 -framerate {FPS} -i \"{BACKGROUND_IMG_PATH}\" -i \"{audio_path}\" \\\n",
        "  -filter_complex \"{filter_complex}\" \\\n",
        "  -map \"[v]\" -map 1:a \\\n",
        "  -c:v libx264 -preset veryfast -pix_fmt yuv420p \\\n",
        "  -c:a aac -b:a 192k -shortest \"{FINAL_MP4}\"\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nğŸ¬ Running ffmpeg to create final video with zooming background + subtitles:\")\n",
        "print(ffmpeg_cmd)\n",
        "\n",
        "process = subprocess.run(\n",
        "    [\"bash\", \"-lc\", ffmpeg_cmd],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "print(\"â”€â”€ ffmpeg output â”€â”€\")\n",
        "print(process.stdout)\n",
        "\n",
        "if process.returncode != 0:\n",
        "    raise RuntimeError(f\"ffmpeg failed with exit code {process.returncode}\")\n",
        "\n",
        "print(\"\\nâœ… Final Shorts video with zooming background + subtitles created!\")\n",
        "print(\"   Path:\", FINAL_MP4)\n"
      ],
      "metadata": {
        "id": "vmW-7xexlqXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 9. Generate high-quality metadata\n",
        "#    - A/B title candidates + heuristic scoring\n",
        "#    - A/B description candidates + heuristic scoring\n",
        "#    - Refined tags\n",
        "#    - Pinned comment\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Configurable series branding: this will be appended to every chosen title\n",
        "SERIES_SUFFIX = \" | Aprenda com Ritmo\"  # tweak this to your channel brand later\n",
        "\n",
        "METADATA_JSON_PATH = RUN_DIR / \"logs\" / \"metadata.json\"\n",
        "\n",
        "# Load lyrics & song meta\n",
        "lyrics_path = RUN_DIR / \"lyrics.txt\"\n",
        "song_meta_path = RUN_DIR / \"logs\" / \"suno_song_meta.json\"\n",
        "\n",
        "if not lyrics_path.exists():\n",
        "    raise FileNotFoundError(f\"lyrics.txt not found at {lyrics_path}. Run Block 4 first.\")\n",
        "if not song_meta_path.exists():\n",
        "    raise FileNotFoundError(f\"suno_song_meta.json not found at {song_meta_path}. Run Block 5 first.\")\n",
        "\n",
        "lyrics_text = lyrics_path.read_text(encoding=\"utf-8\")\n",
        "song_meta = json.loads(song_meta_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "duration_sec = song_meta.get(\"duration\", None)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Helpers: subject keywords\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def extract_subject_keywords(subject: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extract simple keywords from the subject for matching.\n",
        "    Very lightweight: split on spaces, drop super common words.\n",
        "    \"\"\"\n",
        "    stopwords = {\"the\", \"a\", \"an\", \"of\", \"and\", \"for\", \"to\", \"in\", \"vs\", \"very\", \"high\", \"level\", \"high-level\"}\n",
        "    tokens = [w.lower() for w in re.split(r\"[^a-zA-Z0-9]+\", subject) if w]\n",
        "    return [t for t in tokens if t not in stopwords]\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Title scoring heuristics for A/B testing\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def score_title(title: str, subject: str) -> float:\n",
        "    \"\"\"\n",
        "    Simple heuristic score:\n",
        "    - Reward:\n",
        "      - Contains subject keywords\n",
        "      - Reasonable length (<80 chars, sweet spot ~40â€“70)\n",
        "      - Contains benefit words like 'explained', 'finally', 'simple'\n",
        "      - Contains a question mark\n",
        "    - Penalize:\n",
        "      - Hashtags\n",
        "      - Overly long\n",
        "      - All caps or spammy punctuation\n",
        "    \"\"\"\n",
        "    score = 0.0\n",
        "    t = title.strip()\n",
        "    tl = t.lower()\n",
        "\n",
        "    # Length\n",
        "    length = len(t)\n",
        "    if 40 <= length <= 72:\n",
        "        score += 3.0\n",
        "    elif length <= 90:\n",
        "        score += 1.5\n",
        "    else:\n",
        "        score -= 2.0\n",
        "\n",
        "    # Subject keyword coverage\n",
        "    subject_keywords = extract_subject_keywords(subject)\n",
        "    kw_matches = sum(1 for kw in subject_keywords if kw in tl)\n",
        "    score += min(kw_matches * 1.5, 4.0)  # cap keyword bonus\n",
        "\n",
        "    # Benefit / clarity words\n",
        "    benefit_words = [\n",
        "        \"explained\", \"explained simply\", \"finally makes sense\",\n",
        "        \"for beginners\", \"made simple\", \"in plain language\",\n",
        "        \"in simple terms\", \"under 60 seconds\", \"easy breakdown\",\n",
        "        \"learn fast\", \"demystified\", \"step by step\", \"the basics\"\n",
        "    ]\n",
        "    for bw in benefit_words:\n",
        "        if bw in tl:\n",
        "            score += 2.0\n",
        "            break  # don't over-count\n",
        "\n",
        "    # Question mark can increase curiosity\n",
        "    if \"?\" in t:\n",
        "        score += 1.0\n",
        "\n",
        "    # Penalize hashtags or emojis\n",
        "    if \"#\" in t:\n",
        "        score -= 3.0\n",
        "    if any(ch in t for ch in [\"ğŸ”¥\", \"ğŸ’°\", \"ğŸš€\", \"ğŸ¤¯\", \"ğŸ˜±\"]):\n",
        "        score -= 2.0\n",
        "\n",
        "    # Penalize shouty ALL CAPS (ignoring small words)\n",
        "    letters_only = re.sub(r\"[^A-Za-z]+\", \"\", t)\n",
        "    if letters_only and letters_only.isupper():\n",
        "        score -= 3.0\n",
        "\n",
        "    # Penalize too many ! or ? overall\n",
        "    if t.count(\"!\") + t.count(\"?\") > 3:\n",
        "        score -= 1.5\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def pick_best_title(candidates: list[str], subject: str) -> str:\n",
        "    \"\"\"\n",
        "    Score all candidates and pick the best one.\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        raise ValueError(\"No title candidates provided.\")\n",
        "\n",
        "    scored = []\n",
        "    for c in candidates:\n",
        "        c_clean = c.strip()\n",
        "        if not c_clean:\n",
        "            continue\n",
        "        s = score_title(c_clean, subject)\n",
        "        scored.append((s, c_clean))\n",
        "\n",
        "    if not scored:\n",
        "        raise ValueError(\"All title candidates were empty or invalid.\")\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_title = scored[0]\n",
        "\n",
        "    print(\"ğŸ† Title A/B ranking:\")\n",
        "    for s, t in scored:\n",
        "        mark = \" <â€“ chosen\" if t == best_title else \"\"\n",
        "        print(f\"  [{s:+.2f}] {t}{mark}\")\n",
        "\n",
        "    return best_title\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Description scoring heuristics (for A/B testing)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def score_description(desc: str, subject: str) -> float:\n",
        "    \"\"\"\n",
        "    Heuristic description score:\n",
        "    - Hook in the first 1â€“2 sentences\n",
        "    - Contains subject keywords\n",
        "    - Contains CTA words (like, comment, subscribe)\n",
        "    - Ends with a question\n",
        "    - Hashtags in a block near the end\n",
        "    - Reasonable total length (not too short, not a wall of text)\n",
        "    \"\"\"\n",
        "    score = 0.0\n",
        "    d = desc.strip()\n",
        "    dl = d.lower()\n",
        "\n",
        "    # Rough sentence split\n",
        "    sentences = re.split(r\"(?<=[.!?])\\s+\", d)\n",
        "    first_sentence = sentences[0] if sentences else d\n",
        "    first_len = len(first_sentence)\n",
        "\n",
        "    # Hook length\n",
        "    if 40 <= first_len <= 200:\n",
        "        score += 3.0\n",
        "    elif first_len <= 260:\n",
        "        score += 1.0\n",
        "    else:\n",
        "        score -= 1.0\n",
        "\n",
        "    # Subject keywords\n",
        "    subject_keywords = extract_subject_keywords(subject)\n",
        "    kw_matches = sum(1 for kw in subject_keywords if kw in dl)\n",
        "    score += min(kw_matches * 1.0, 4.0)\n",
        "\n",
        "    # CTA presence\n",
        "    for word in [\"like\", \"comment\", \"subscribe\", \"share\"]:\n",
        "        if word in dl:\n",
        "            score += 1.5\n",
        "            break\n",
        "\n",
        "    # Question at the end (for engagement)\n",
        "    tail = d[-200:]\n",
        "    if \"?\" in tail:\n",
        "        score += 2.0\n",
        "\n",
        "    # Hashtags near the end, not spammed at the start\n",
        "    hash_positions = [m.start() for m in re.finditer(r\"#\", d)]\n",
        "    if hash_positions:\n",
        "        # Check if majority of hashtags are in last 1/3 of text\n",
        "        last_third = len(d) * (2.0 / 3.0)\n",
        "        tail_hashes = sum(1 for pos in hash_positions if pos >= last_third)\n",
        "        if tail_hashes >= max(1, len(hash_positions) // 2):\n",
        "            score += 2.0\n",
        "        else:\n",
        "            score -= 1.0\n",
        "\n",
        "    # Overall length\n",
        "    total_len = len(d)\n",
        "    if 200 <= total_len <= 1500:\n",
        "        score += 2.0\n",
        "    elif total_len > 2500:\n",
        "        score -= 1.5\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def pick_best_description(candidates: list[str], subject: str) -> str:\n",
        "    \"\"\"\n",
        "    Score all description candidates and pick the best one.\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        raise ValueError(\"No description candidates provided.\")\n",
        "\n",
        "    scored = []\n",
        "    for c in candidates:\n",
        "        c_clean = c.strip()\n",
        "        if not c_clean:\n",
        "            continue\n",
        "        s = score_description(c_clean, subject)\n",
        "        scored.append((s, c_clean))\n",
        "\n",
        "    if not scored:\n",
        "        raise ValueError(\"All description candidates were empty or invalid.\")\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_score, best_desc = scored[0]\n",
        "\n",
        "    print(\"ğŸ† Description A/B ranking (scores only):\")\n",
        "    for s, _ in scored:\n",
        "        mark = \" <â€“ chosen\" if _ == best_desc else \"\"\n",
        "        print(f\"  [{s:+.2f}]{mark}\")\n",
        "\n",
        "    return best_desc\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# GPT metadata generator (multi-title + multi-description)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def generate_video_metadata(\n",
        "    subject: str,\n",
        "    persona: str,\n",
        "    tone: str,\n",
        "    lyrics: str,\n",
        "    duration_sec: float | None,\n",
        "    language: str = LYRICS_LANGUAGE,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Use GPT to generate:\n",
        "    - multiple title candidates (for A/B testing)\n",
        "    - multiple description candidates (for A/B testing)\n",
        "    - tags (SEO keywords, no '#')\n",
        "    - pinned_comment (for manual pinning to boost engagement)\n",
        "    Returns a dict with:\n",
        "      - title: chosen best title (with suffix)\n",
        "      - title_candidates: [... all raw candidates ...]\n",
        "      - description: chosen best description\n",
        "      - description_candidates: [...]\n",
        "      - tags: refined tags\n",
        "      - pinned_comment: str\n",
        "    \"\"\"\n",
        "    duration_hint = \"\"\n",
        "    if duration_sec:\n",
        "        duration_hint = f\"The song is about {int(duration_sec)} seconds long. \"\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are an expert YouTube Shorts growth strategist and copywriter.\\n\\n\"\n",
        "        f\"Video details:\\n\"\n",
        "        f\"- Language: {language}\\n\"\n",
        "        f\"- Subject: {subject}\\n\"\n",
        "        f\"- Musical persona/style: {persona}\\n\"\n",
        "        f\"- Emotional tone: {tone}\\n\"\n",
        "        f\"- {duration_hint}\\n\"\n",
        "        \"The video is an educational song that explains the subject in a fun, catchy way.\\n\\n\"\n",
        "        \"You will receive the full song lyrics. Use them ONLY as inspiration for tone and angle, \"\n",
        "        \"not to copy line-by-line.\\n\\n\"\n",
        "        \"Your job:\\n\"\n",
        "        \"1) Create 5 STRONG, professional-looking TITLE CANDIDATES that:\\n\"\n",
        "        \"   - Feel like a human made them, not AI.\\n\"\n",
        "        \"   - Do NOT contain hashtags or emojis.\\n\"\n",
        "        \"   - Put the main keyword (subject) near the beginning.\\n\"\n",
        "        \"   - Each uses a slightly different angle (curiosity, benefit, emotional, question, etc.).\\n\"\n",
        "        f\"   - Each should be written WITHOUT the series suffix. I will append \\\"{SERIES_SUFFIX}\\\" myself.\\n\"\n",
        "        \"   - Maximum ~80 characters each.\\n\"\n",
        "        \"2) Create 3 DESCRIPTION CANDIDATES that:\\n\"\n",
        "        \"   - Each starts with a strong 1â€“2 sentence hook about the subject and what viewers will learn.\\n\"\n",
        "        \"   - Each includes a short paragraph (3â€“5 sentences) summarizing what the song explains.\\n\"\n",
        "        \"   - Each includes a call to action to LIKE, COMMENT and SUBSCRIBE.\\n\"\n",
        "        \"   - Each ends with a question that invites comments (e.g., what should we cover next?).\\n\"\n",
        "        \"   - After the explanatory text, add a blank line, then a block of 10â€“20 relevant HASHTAGS \"\n",
        "        \"(with '#', for example #shorts #education #physics). Include #shorts and #youtubeshorts.\\n\"\n",
        "        \"   - The descriptions can vary the hook angle (curiosity, relatable pain, promise of clarity, etc.).\\n\"\n",
        "        \"3) Create a TAGS list (YouTube 'tags' / keywords, NOT hashtags):\\n\"\n",
        "        \"   - 20â€“30 items.\\n\"\n",
        "        \"   - No '#' characters.\\n\"\n",
        "        \"   - Mix of exact subject phrases, search-style queries, and general edutainment keywords.\\n\"\n",
        "        \"   - All in the same language as the titles/descriptions.\\n\"\n",
        "        \"4) Create a PINNED COMMENT suggestion:\\n\"\n",
        "        \"   - 1â€“3 short sentences.\\n\"\n",
        "        \"   - Encourages viewers to comment what topic they want songified next.\\n\"\n",
        "        \"   - Optionally references a key question from the video.\\n\"\n",
        "        \"   - No hashtags.\\n\\n\"\n",
        "        \"VERY IMPORTANT:\\n\"\n",
        "        f\"- Title candidates: no hashtags, no emojis, do NOT include the suffix \\\"{SERIES_SUFFIX}\\\".\\n\"\n",
        "        \"- Descriptions: hashtags only at the end in a block, not in the opening hook.\\n\"\n",
        "        \"- Tags: no '#', just plain keywords.\\n\\n\"\n",
        "        \"Return your answer as a VALID JSON object with this exact structure:\\n\"\n",
        "        \"{\\n\"\n",
        "        \"  \\\"title_candidates\\\": [\\\"...\\\", \\\"...\\\", \\\"...\\\", \\\"...\\\", \\\"...\\\"],\\n\"\n",
        "        \"  \\\"description_candidates\\\": [\\\"...\\\", \\\"...\\\", \\\"...\\\"],\\n\"\n",
        "        \"  \\\"tags\\\": [\\\"tag1\\\", \\\"tag2\\\", \\\"tag3\\\", ...],\\n\"\n",
        "        \"  \\\"pinned_comment\\\": \\\"...\\\"\\n\"\n",
        "        \"}\\n\\n\"\n",
        "        \"Here are the lyrics for context:\\n\"\n",
        "        \"----- LYRICS START -----\\n\"\n",
        "        f\"{lyrics}\\n\"\n",
        "        \"----- LYRICS END -----\\n\"\n",
        "    )\n",
        "\n",
        "    resp = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.8,\n",
        "        max_tokens=1500,\n",
        "    )\n",
        "\n",
        "    raw = resp.choices[0].message.content.strip()\n",
        "\n",
        "    # Try to parse as JSON\n",
        "    try:\n",
        "        meta = json.loads(raw)\n",
        "    except json.JSONDecodeError:\n",
        "        match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(f\"Metadata response was not valid JSON:\\n{raw}\")\n",
        "        meta = json.loads(match.group(0))\n",
        "\n",
        "    # Basic sanity checks\n",
        "    if \"tags\" not in meta:\n",
        "        raise ValueError(f\"Metadata JSON missing required field 'tags': {meta.keys()}\")\n",
        "\n",
        "    # â”€â”€ Titles: get candidates, clean, pick best â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    title_candidates = meta.get(\"title_candidates\") or []\n",
        "    if isinstance(title_candidates, str):\n",
        "        title_candidates = [title_candidates]\n",
        "    if not title_candidates and \"title\" in meta:\n",
        "        title_candidates = [meta[\"title\"]]\n",
        "\n",
        "    cleaned_title_candidates = []\n",
        "    for c in title_candidates:\n",
        "        c_str = str(c).strip()\n",
        "        if not c_str:\n",
        "            continue\n",
        "        c_str = c_str.replace(\"{SERIES_SUFFIX}\", \"\").strip()\n",
        "        c_str = c_str.replace(\"#\", \"\").strip()\n",
        "        cleaned_title_candidates.append(c_str)\n",
        "\n",
        "    if not cleaned_title_candidates:\n",
        "        raise ValueError(\"No usable title candidates after cleaning.\")\n",
        "\n",
        "    chosen_title = pick_best_title(cleaned_title_candidates, subject)\n",
        "\n",
        "    # Append series suffix if not already present\n",
        "    if SERIES_SUFFIX.strip().lower() not in chosen_title.lower():\n",
        "        final_title = f\"{chosen_title}{SERIES_SUFFIX}\"\n",
        "    else:\n",
        "        final_title = chosen_title\n",
        "\n",
        "    # â”€â”€ Descriptions: get candidates, pick best â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    desc_candidates = meta.get(\"description_candidates\") or []\n",
        "    if isinstance(desc_candidates, str):\n",
        "        desc_candidates = [desc_candidates]\n",
        "    if not desc_candidates and \"description\" in meta:\n",
        "        desc_candidates = [meta[\"description\"]]\n",
        "\n",
        "    cleaned_desc_candidates = []\n",
        "    for d in desc_candidates:\n",
        "        d_str = str(d).strip()\n",
        "        if not d_str:\n",
        "            continue\n",
        "        cleaned_desc_candidates.append(d_str)\n",
        "\n",
        "    if not cleaned_desc_candidates:\n",
        "        raise ValueError(\"No usable description candidates after cleaning.\")\n",
        "\n",
        "    chosen_description = pick_best_description(cleaned_desc_candidates, subject)\n",
        "\n",
        "    # â”€â”€ Tags: refine & enrich â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    tags = meta.get(\"tags\", [])\n",
        "    clean_tags = []\n",
        "    seen = set()\n",
        "\n",
        "    for t in tags:\n",
        "        t_clean = str(t).strip()\n",
        "        if not t_clean:\n",
        "            continue\n",
        "        if t_clean.startswith(\"#\"):\n",
        "            t_clean = t_clean[1:]\n",
        "        t_clean = re.sub(r\"\\s+\", \" \", t_clean).strip()\n",
        "        t_lower = t_clean.lower()\n",
        "        if t_lower not in seen:\n",
        "            seen.add(t_lower)\n",
        "            clean_tags.append(t_clean)\n",
        "\n",
        "    # Enrich with subject-based SEO tags\n",
        "    subject_keywords = extract_subject_keywords(subject)\n",
        "    base_subject_tags = [\n",
        "        subject,\n",
        "        f\"{subject} explained\",\n",
        "        f\"{subject} for beginners\",\n",
        "        f\"{subject} made simple\",\n",
        "        f\"{subject} song\",\n",
        "        f\"{subject} tutorial\",\n",
        "    ]\n",
        "    for kw in subject_keywords:\n",
        "        base_subject_tags.append(f\"{kw} basics\")\n",
        "        base_subject_tags.append(f\"what is {kw}\")\n",
        "    for t in base_subject_tags:\n",
        "        t_clean = t.strip()\n",
        "        if not t_clean:\n",
        "            continue\n",
        "        t_lower = t_clean.lower()\n",
        "        if t_lower not in seen:\n",
        "            seen.add(t_lower)\n",
        "            clean_tags.append(t_clean)\n",
        "\n",
        "    # Limit total number of tags to something reasonable (upload step further sanitizes)\n",
        "    clean_tags = clean_tags[:40]\n",
        "\n",
        "    # â”€â”€ Pinned comment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    pinned_comment = meta.get(\"pinned_comment\", \"\") or \"\"\n",
        "    if not pinned_comment.strip():\n",
        "        pinned_comment = (\n",
        "            \"What topic should I turn into a song next? \"\n",
        "            \"Drop your ideas in the comments!\"\n",
        "        )\n",
        "\n",
        "    # Build final metadata dict\n",
        "    final_meta = {\n",
        "        \"title\": final_title,\n",
        "        \"title_candidates\": cleaned_title_candidates,\n",
        "        \"description\": chosen_description,\n",
        "        \"description_candidates\": cleaned_desc_candidates,\n",
        "        \"tags\": clean_tags,\n",
        "        \"pinned_comment\": pinned_comment,\n",
        "    }\n",
        "\n",
        "    return final_meta\n",
        "\n",
        "\n",
        "# â”€â”€ Generate & save metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print(\"ğŸ§¾ Generating YouTube metadata with GPT (A/B titles & descriptions)...\")\n",
        "video_meta = generate_video_metadata(\n",
        "    subject=subject,\n",
        "    persona=persona,\n",
        "    tone=tone,\n",
        "    lyrics=lyrics_text,\n",
        "    duration_sec=duration_sec,\n",
        "    language=LYRICS_LANGUAGE,\n",
        ")\n",
        "\n",
        "METADATA_JSON_PATH.write_text(json.dumps(video_meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Metadata generated and saved to:\", METADATA_JSON_PATH)\n",
        "print(\"\\nâ”€â”€â”€â”€ FINAL CHOSEN TITLE â”€â”€â”€â”€\")\n",
        "print(video_meta[\"title\"])\n",
        "print(\"\\nâ”€â”€â”€â”€ TITLE CANDIDATES â”€â”€â”€â”€\")\n",
        "for i, t in enumerate(video_meta.get(\"title_candidates\", []), start=1):\n",
        "    print(f\"{i}. {t}\")\n",
        "print(\"\\nâ”€â”€â”€â”€ DESCRIPTION (preview) â”€â”€â”€â”€\")\n",
        "print(video_meta[\"description\"][:800], \"...\\n\")\n",
        "print(\"\\nâ”€â”€â”€â”€ TAGS (first 20) â”€â”€â”€â”€\")\n",
        "print(video_meta[\"tags\"][:20])\n",
        "print(\"\\nâ”€â”€â”€â”€ PINNED COMMENT SUGGESTION â”€â”€â”€â”€\")\n",
        "print(video_meta[\"pinned_comment\"])\n"
      ],
      "metadata": {
        "id": "wK0-3qY8pGA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 10. Upload final video to YouTube (with robust tag fallback)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# We reuse:\n",
        "# - config (for youtube_privacy)\n",
        "# - get_authenticated_service (from Block 2)\n",
        "# - RUN_DIR\n",
        "# - FINAL_MP4 (from Block 8)\n",
        "# - METADATA_JSON_PATH (from Block 9)\n",
        "# - subject (from Block 3)\n",
        "\n",
        "# Sanity checks\n",
        "if not FINAL_MP4.exists():\n",
        "    raise FileNotFoundError(f\"Final video not found at {FINAL_MP4}. Run Block 8 first.\")\n",
        "\n",
        "if not METADATA_JSON_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Metadata JSON not found at {METADATA_JSON_PATH}. Run Block 9 first.\")\n",
        "\n",
        "video_meta = json.loads(METADATA_JSON_PATH.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "title = video_meta[\"title\"]\n",
        "description = video_meta[\"description\"]\n",
        "tags = video_meta.get(\"tags\", [])\n",
        "pinned_comment_suggestion = video_meta.get(\"pinned_comment\", \"\")\n",
        "\n",
        "# Ensure tags is a list of strings\n",
        "if isinstance(tags, str):\n",
        "    tags = [t.strip() for t in tags.split(\",\") if t.strip()]\n",
        "\n",
        "\n",
        "def get_youtube_client_safe():\n",
        "    \"\"\"\n",
        "    Reuse global youtube_client if it exists, otherwise create a new one.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        yt = youtube_client  # from preflight, if still in memory\n",
        "        _ = yt.videos  # simple sanity check\n",
        "        print(\"ğŸ” Reusing existing YouTube client.\")\n",
        "        return yt\n",
        "    except Exception:\n",
        "        print(\"ğŸ”„ Creating new YouTube client via get_authenticated_service()...\")\n",
        "        return get_authenticated_service()\n",
        "\n",
        "\n",
        "def clean_youtube_tags(raw_tags):\n",
        "    \"\"\"\n",
        "    Strong cleaning so YouTube is less likely to reject tags.\n",
        "\n",
        "    Strategy:\n",
        "    - Remove '#'\n",
        "    - Keep only letters, digits, spaces, and a small safe set of punctuation: - & ' ! ? .\n",
        "    - Truncate each tag to 30 characters\n",
        "    - Deduplicate (case-insensitive)\n",
        "    - Trim list so total length <= 450 chars\n",
        "    \"\"\"\n",
        "    allowed_pattern = re.compile(r\"[^a-zA-Z0-9 \\-\\&\\'!\\?\\.]\", re.UNICODE)\n",
        "\n",
        "    cleaned = []\n",
        "    seen = set()\n",
        "    total_len = 0\n",
        "    MAX_TAG_LEN = 30\n",
        "    MAX_TOTAL_LEN = 450\n",
        "\n",
        "    for t in raw_tags:\n",
        "        t = str(t).strip()\n",
        "        if not t:\n",
        "            continue\n",
        "\n",
        "        # Remove leading '#'\n",
        "        if t.startswith(\"#\"):\n",
        "            t = t[1:].strip()\n",
        "        if not t:\n",
        "            continue\n",
        "\n",
        "        # Remove disallowed chars\n",
        "        t = allowed_pattern.sub(\"\", t)\n",
        "\n",
        "        # Collapse multiple spaces\n",
        "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "        if not t:\n",
        "            continue\n",
        "\n",
        "        # Truncate\n",
        "        if len(t) > MAX_TAG_LEN:\n",
        "            t = t[:MAX_TAG_LEN].rstrip()\n",
        "\n",
        "        t_lower = t.lower()\n",
        "        if t_lower in seen:\n",
        "            continue\n",
        "\n",
        "        # Check total length budget\n",
        "        if total_len + len(t) > MAX_TOTAL_LEN:\n",
        "            break\n",
        "\n",
        "        cleaned.append(t)\n",
        "        seen.add(t_lower)\n",
        "        total_len += len(t)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def upload_video_to_youtube_attempt(\n",
        "    youtube,\n",
        "    file_path,\n",
        "    title,\n",
        "    description,\n",
        "    tags_attempt,\n",
        "    category_id=\"27\",  # 'Education'\n",
        "    privacy=\"public\",\n",
        "    chunk_size=1024 * 1024,\n",
        "    max_retries=5,\n",
        "    label=\"\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Single upload attempt with a specific tag set.\n",
        "    Raises HttpError if YouTube rejects tags or other non-transient errors.\n",
        "    \"\"\"\n",
        "    # Ensure #shorts in description for Shorts distribution\n",
        "    if \"#shorts\" not in description.lower():\n",
        "        description_to_send = description.rstrip() + \"\\n\\n#shorts\"\n",
        "    else:\n",
        "        description_to_send = description\n",
        "\n",
        "    print(f\"\\nğŸ”– Attempt '{label}': sending tags:\")\n",
        "    print(tags_attempt)\n",
        "    print(\"Total chars in tags:\", sum(len(t) for t in tags_attempt))\n",
        "\n",
        "    snippet = {\n",
        "        \"title\": title,\n",
        "        \"description\": description_to_send,\n",
        "        \"categoryId\": str(category_id),\n",
        "    }\n",
        "\n",
        "    # Only include tags field if we actually have any\n",
        "    if tags_attempt:\n",
        "        snippet[\"tags\"] = tags_attempt\n",
        "\n",
        "    body = {\n",
        "        \"snippet\": snippet,\n",
        "        \"status\": {\n",
        "            \"privacyStatus\": privacy,\n",
        "            \"selfDeclaredMadeForKids\": False,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    media = MediaFileUpload(\n",
        "        str(file_path),\n",
        "        mimetype=\"video/mp4\",\n",
        "        chunksize=chunk_size,\n",
        "        resumable=True,\n",
        "    )\n",
        "\n",
        "    request = youtube.videos().insert(\n",
        "        part=\"snippet,status\",\n",
        "        body=body,\n",
        "        media_body=media,\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸš€ Starting YouTube upload (attempt '{label}')...\")\n",
        "    response = None\n",
        "    retry = 0\n",
        "\n",
        "    while response is None:\n",
        "        try:\n",
        "            status, response = request.next_chunk()\n",
        "            if status:\n",
        "                print(f\"ğŸŸ¢ Upload progress: {int(status.progress() * 100)}%\")\n",
        "        except HttpError as e:\n",
        "            # invalidTags error is handled by the caller (for fallback)\n",
        "            if e.resp.status == 400 and \"invalidTags\" in str(e):\n",
        "                print(f\"âŒ YouTube reports invalid tags on attempt '{label}'.\")\n",
        "                raise\n",
        "\n",
        "            # Transient server errors â†’ exponential backoff\n",
        "            if e.resp.status in [500, 502, 503, 504] and retry < max_retries:\n",
        "                retry += 1\n",
        "                wait_for = 2 ** retry\n",
        "                print(f\"âš ï¸ Transient YouTube error ({e.resp.status}). Retry {retry}/{max_retries} in {wait_for}s...\")\n",
        "                time.sleep(wait_for)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"âŒ Non-recoverable YouTube error on attempt '{label}': {e}\")\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            if retry < max_retries:\n",
        "                retry += 1\n",
        "                wait_for = 2 ** retry\n",
        "                print(f\"âš ï¸ Unexpected error during upload (attempt '{label}'): {e}. \"\n",
        "                      f\"Retry {retry}/{max_retries} in {wait_for}s...\")\n",
        "                time.sleep(wait_for)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"âŒ Max retries reached on attempt '{label}'. Upload failed.\")\n",
        "                raise\n",
        "\n",
        "    if \"id\" not in response:\n",
        "        raise RuntimeError(f\"Unexpected YouTube API response on attempt '{label}': {response}\")\n",
        "\n",
        "    video_id = response[\"id\"]\n",
        "    print(f\"âœ… Upload complete on attempt '{label}'! Video ID:\", video_id)\n",
        "    return video_id\n",
        "\n",
        "\n",
        "def upload_video_to_youtube_with_fallback(\n",
        "    youtube,\n",
        "    file_path,\n",
        "    title,\n",
        "    description,\n",
        "    tags,\n",
        "    category_id=\"27\",\n",
        "    privacy=\"public\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Try uploading with:\n",
        "    1) Full cleaned tags\n",
        "    2) If invalidTags: small conservative tag set\n",
        "    3) If still invalidTags: no tags at all\n",
        "\n",
        "    This guarantees that tags can never break the entire run.\n",
        "    \"\"\"\n",
        "    # 1) Full cleaned tags\n",
        "    full_cleaned = clean_youtube_tags(tags)\n",
        "\n",
        "    try:\n",
        "        return upload_video_to_youtube_attempt(\n",
        "            youtube=youtube,\n",
        "            file_path=file_path,\n",
        "            title=title,\n",
        "            description=description,\n",
        "            tags_attempt=full_cleaned,\n",
        "            category_id=category_id,\n",
        "            privacy=privacy,\n",
        "            label=\"full-cleaned-tags\",\n",
        "        )\n",
        "    except HttpError as e:\n",
        "        if e.resp.status == 400 and \"invalidTags\" in str(e):\n",
        "            print(\"âš ï¸ YouTube rejected the full tag set (invalidTags). Falling back to conservative tags...\")\n",
        "        else:\n",
        "            # Some other error; re-raise\n",
        "            raise\n",
        "\n",
        "    # 2) Conservative tag set, very safe\n",
        "    conservative_raw = [\n",
        "        subject,\n",
        "        f\"{subject} explained\",\n",
        "        \"educational song\",\n",
        "        \"learning with music\",\n",
        "        \"explained simply\",\n",
        "    ]\n",
        "    conservative_cleaned = clean_youtube_tags(conservative_raw)\n",
        "\n",
        "    try:\n",
        "        return upload_video_to_youtube_attempt(\n",
        "            youtube=youtube,\n",
        "            file_path=file_path,\n",
        "            title=title,\n",
        "            description=description,\n",
        "            tags_attempt=conservative_cleaned,\n",
        "            category_id=category_id,\n",
        "            privacy=privacy,\n",
        "            label=\"conservative-tags\",\n",
        "        )\n",
        "    except HttpError as e:\n",
        "        if e.resp.status == 400 and \"invalidTags\" in str(e):\n",
        "            print(\"âš ï¸ YouTube rejected even conservative tags (invalidTags). \"\n",
        "                  \"Final fallback: uploading with NO tags.\")\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    # 3) Final fallback: no tags at all\n",
        "    return upload_video_to_youtube_attempt(\n",
        "        youtube=youtube,\n",
        "        file_path=file_path,\n",
        "        title=title,\n",
        "        description=description,\n",
        "        tags_attempt=[],  # no tags\n",
        "        category_id=category_id,\n",
        "        privacy=privacy,\n",
        "        label=\"no-tags\",\n",
        "    )\n",
        "\n",
        "\n",
        "# â”€â”€ Run the upload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "youtube = get_youtube_client_safe()\n",
        "privacy = config.youtube_privacy  # from Block 1 config\n",
        "\n",
        "video_id = upload_video_to_youtube_with_fallback(\n",
        "    youtube=youtube,\n",
        "    file_path=FINAL_MP4,\n",
        "    title=title,\n",
        "    description=description,\n",
        "    tags=tags,\n",
        "    category_id=\"27\",  # Education\n",
        "    privacy=privacy,\n",
        ")\n",
        "\n",
        "video_url = f\"https://youtu.be/{video_id}\"\n",
        "print(\"\\nğŸ‰ Video is live (or queued) on YouTube:\")\n",
        "print(\"   \", video_url)\n",
        "\n",
        "# Show pinned comment suggestion again for easy copy-paste\n",
        "if pinned_comment_suggestion:\n",
        "    pinned_path = RUN_DIR / \"logs\" / \"pinned_comment.txt\"\n",
        "    pinned_path.write_text(pinned_comment_suggestion, encoding=\"utf-8\")\n",
        "    print(\"\\nğŸ“Œ Pinned comment suggestion (also saved to pinned_comment.txt):\")\n",
        "    print(pinned_comment_suggestion)\n",
        "    print(\"\\n   File:\", pinned_path)\n"
      ],
      "metadata": {
        "id": "UMvHP9lEplVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
